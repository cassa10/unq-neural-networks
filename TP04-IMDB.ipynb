{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1yZaCtCpk5AT9Dp6HWhZ163wREo0j-pnj","timestamp":1694474383166},{"file_id":"1Z2TeQ-cS8syVb81wxTYTJxPgvDopOabr","timestamp":1694229142757}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# IMDB reviews"],"metadata":{"id":"HAkz--ueTIvb"}},{"cell_type":"markdown","source":["## Import libraries"],"metadata":{"id":"OoXguueAqSqp"}},{"cell_type":"code","source":["!pip install portalocker"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1apOxnq586q","executionInfo":{"status":"ok","timestamp":1695477646033,"user_tz":180,"elapsed":4435,"user":{"displayName":"Jose Cassano","userId":"06274683169262378241"}},"outputId":"a55f990e-741f-4d48-ee29-8a68c1bc9255"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.8.2)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"moTq3Cc_TFaI","executionInfo":{"status":"ok","timestamp":1695477649487,"user_tz":180,"elapsed":3456,"user":{"displayName":"Jose Cassano","userId":"06274683169262378241"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam\n","from torchtext.datasets import IMDB\n","import gensim.downloader as api\n","from gensim.utils import simple_preprocess\n","import numpy as np"]},{"cell_type":"markdown","source":["## Load pre-trained word2vec embeddings"],"metadata":{"id":"_eyIvFy20Tki"}},{"cell_type":"code","source":["word2vec = api.load(\"word2vec-google-news-300\")"],"metadata":{"id":"Ktci4NH50TKF","executionInfo":{"status":"ok","timestamp":1695477706345,"user_tz":180,"elapsed":56860,"user":{"displayName":"Jose Cassano","userId":"06274683169262378241"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Load the dataset"],"metadata":{"id":"5Bg9l1fGIMBC"}},{"cell_type":"code","source":["#TODO: Revisar porque train no carga bien\n","\n","train_dataset = IMDB(root='./dataset', split='train')\n","test_dataset = IMDB(root='./dataset', split='test')\n","\n","print(len(list(train_dataset)))\n","print(len(list(test_dataset)))"],"metadata":{"id":"YPaWkzIvIPnC","executionInfo":{"status":"ok","timestamp":1695477708559,"user_tz":180,"elapsed":2223,"user":{"displayName":"Jose Cassano","userId":"06274683169262378241"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc19ac0b-4081-4b27-a1ee-d36427ab9bc9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["25000\n","25000\n"]}]},{"cell_type":"markdown","source":["## Visualize the dataset"],"metadata":{"id":"Wf-onxaGIwbf"}},{"cell_type":"code","source":["for i, (label, text) in enumerate(train_dataset):\n","    if i < 10:\n","      print(f\"{i} Label: {label}\\tText: {text[:300]}...\")\n","    if label == 2:\n","      print(f\"{i} Label: {label}\\tText: {text[:300]}...\")\n","#    if i == 200:\n","      break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SrqzsijFI0QB","executionInfo":{"status":"ok","timestamp":1695477709077,"user_tz":180,"elapsed":521,"user":{"displayName":"Jose Cassano","userId":"06274683169262378241"}},"outputId":"2b7d093c-ba70-4bc3-ec3b-05bef194c960"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["0 Label: 1\tText: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really h...\n","1 Label: 1\tText: \"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity...\n","2 Label: 1\tText: If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away wit...\n","3 Label: 1\tText: This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of sill...\n","4 Label: 1\tText: Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />\"Is that all there is??\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \"Good...\n","5 Label: 1\tText: I would put this at the top of my list of films in the category of unwatchable trash! There are films that are bad, but the worst kind are the ones that are unwatchable but you are suppose to like them because they are supposed to be good for you! The sex sequences, so shocking in its day, couldn't ...\n","6 Label: 1\tText: Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual er...\n","7 Label: 1\tText: When I first saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel York's portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think about a legend like Lucille Ball being portrayed the way she...\n","8 Label: 1\tText: Who are these \"They\"- the actors? the filmmakers? Certainly couldn't be the audience- this is among the most air-puffed productions in existence. It's the kind of movie that looks like it was a lot of fun to shoot TOO much fun, nobody is getting any actual work done, and that almost always makes fo...\n","9 Label: 1\tText: This is said to be a personal film for Peter Bogdonavitch. He based it on his life but changed things around to fit the characters, who are detectives. These detectives date beautiful models and have no problem getting them. Sounds more like a millionaire playboy filmmaker than a detective, doesn't ...\n","12500 Label: 2\tText: Zentropa has much in common with The Third Man, another noir-like film set among the rubble of postwar Europe. Like TTM, there is much inventive camera work. There is an innocent American who gets emotionally involved with a woman he doesn't really understand, and whose naivety is all the more strik...\n"]}]},{"cell_type":"markdown","source":["## Define our processed dataset"],"metadata":{"id":"OHDR9NO7J14G"}},{"cell_type":"code","source":["class IMDBTensorDataset(Dataset):\n","\n","    def __init__(self, dataset, word2vec):\n","        self.dataset = self._preprocess(dataset, word2vec)\n","\n","    def _preprocess(self, dataset, word2vec):\n","        result = []\n","        for label, text in dataset:\n","            tokens = simple_preprocess(text)\n","            text_vectors = [\n","                word2vec[token]\n","                for token in tokens if token in word2vec\n","            ]\n","            # evalua si hay reviews vacias o cortas, y se generan matrices de 0s (en el else)\n","            if text_vectors:\n","                text_arrays = np.array(text_vectors)\n","                # tomar un tensor con todas las palabras/embeddings calculando su promedio con mean\n","                text_tensor = torch.tensor(text_arrays).mean(0)\n","            else:\n","                # El 300 es porque el word2vec importado usa \"300\", ej: \"word2vec-google-news-300\"\n","                text_tensor = torch.zero(300)\n","            label_normalized = label - 1\n","            result.append((label_normalized, text_tensor))\n","        return result\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]"],"metadata":{"id":"NikQ6LE2J7gm","executionInfo":{"status":"ok","timestamp":1695477709077,"user_tz":180,"elapsed":3,"user":{"displayName":"Jose Cassano","userId":"06274683169262378241"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Create data loaders"],"metadata":{"id":"2Wf8vjw3MtYV"}},{"cell_type":"code","source":["train_tensor_dataset = IMDBTensorDataset(train_dataset, word2vec)\n","# drop_last descarta si el ultimo batch no llega a 64\n","train_loader = DataLoader(train_tensor_dataset, batch_size=64, shuffle=True, drop_last=True)\n","\n","test_tensor_dataset = IMDBTensorDataset(test_dataset, word2vec)\n","test_loader = DataLoader(test_tensor_dataset, batch_size=64, shuffle=False, drop_last=True)"],"metadata":{"id":"HbJ2EqVMMwJq","executionInfo":{"status":"ok","timestamp":1695477760806,"user_tz":180,"elapsed":51731,"user":{"displayName":"Jose Cassano","userId":"06274683169262378241"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Define the model"],"metadata":{"id":"CfFcryjJNpkz"}},{"cell_type":"code","source":["# input_dim = tamaño del embedding (300)\n","# output_dim = 1 porque estamos intentando encontrar la clase binaria (0 -> negativo | 1  -> positivo)\n","\n","\n","# Se usa \"nn.ReLU\" y no la lib \"F\" porque sino habria que manejar la composicion de capas en el forward y no todo desde el nn.Sequential(...)\n","# Sigmoid porque queremos que este entre 0 y 1 (por la clasificacion binaria)\n","\n","class MLP(nn.Module):\n","\n","    def __init__(self, hidden_dim, input_dim=300, output_dim=1):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(hidden_dim, output_dim),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)"],"metadata":{"id":"95vMS4c0NwVo","executionInfo":{"status":"ok","timestamp":1695477760807,"user_tz":180,"elapsed":10,"user":{"displayName":"Jose Cassano","userId":"06274683169262378241"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Train the model"],"metadata":{"id":"WrxcISgAO48T"}},{"cell_type":"code","source":["from torch.cuda import is_available\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#epochs = 10\n","#lr = 0.001\n","#hidden_dim = 128\n","\n","def train_model(epochs, lr, hidden_dim):\n","  # Less value of loss function is the best model??\n","  model = MLP(hidden_dim).to(device)\n","\n","  # BCE = Binary Cross Entropy Loss\n","  criterion = nn.BCELoss()\n","  optimizer = Adam(model.parameters(), lr=lr)\n","  score = 0.0\n","\n","  for epoch in range(epochs):\n","      running_loss = 0.0\n","      running_loss_test = 0.0\n","\n","      #Train\n","      for labels, texts in train_loader:\n","          labels = labels.float().to(device).unsqueeze(1)\n","          texts = texts.to(device)\n","          optimizer.zero_grad()\n","          predictions = model(texts)\n","          loss = criterion(predictions, labels)\n","          loss.backward()\n","          optimizer.step()\n","          running_loss += loss.item()\n","\n","      # Validation (cambiar de test_loader a validation_loader, y este validation_loader debe ser un split del train_loader porque sino overfittea los hiperparametros)\n","      for t_labels, t_texts in test_loader:\n","          t_labels = t_labels.float().to(device).unsqueeze(1)\n","          t_texts = t_texts.to(device)\n","          t_predictions = model(t_texts)\n","          loss_t = criterion(t_predictions, t_labels)\n","          running_loss_test += loss_t.item()\n","\n","      # Get score comparing train vs test loss\n","      avg_train_loss = running_loss / len(train_loader)\n","      avg_test_loss = running_loss_test / len(test_loader)\n","\n","      print(f\"Epoch [{epoch+1}/{epochs}] | Loss Train: {avg_train_loss:.6f}, Loss Test: {avg_test_loss:.6f}\")\n","\n","  # Score (Se utiliza accuracy para obtener mejor modelo porque si se utiliza la loss retorna el modelo que mas overfittea)\n","  with torch.no_grad():\n","    correct, total = 0, 0\n","\n","    for labels, texts in test_loader:\n","        labels = labels.to(device).unsqueeze(1)\n","        texts = texts.to(device)\n","        outputs = model(texts)\n","        predictions = torch.round(outputs.data)\n","        total += labels.size(0)\n","        correct += (predictions == labels).sum().item()\n","    score = correct / total\n","\n","  return (score, model)\n","\n","# Hyper-parameters tuning\n","\n","# Grid-Search\n","ls_epochs = [10, 25, 50]\n","ls_lrs = [0.1, 0.01, 0.001]\n","ls_hidden_dims = [32, 64, 128, 256]\n","\n","grid_search =  [(e, l, h) for e in ls_epochs for l in ls_lrs for h in ls_hidden_dims]\n","\n","print(grid_search)\n","\n","# score_models len must be equal grid_search = [(epoch, lr, hidden_dim)]\n","best_model = None\n","# best_score is best accuracy found\n","best_score = 0\n","\n","# For debug\n","best_comb = None\n","\n","for i, (epochs, lr, hidden_dim) in enumerate(grid_search):\n","  tmp_score, tmp_model = train_model(epochs, lr, hidden_dim)\n","  print(f'[{i+1}/{len(grid_search)}] (Epochs: {epochs}, Lr: {lr}, Hidden_dim: {hidden_dim}) =>')\n","  #print(f\"Score Accuracy: { correct / total:.2f}%\")\n","  print(f'   actual best_model_score: {best_score} - train_score: {tmp_score}')\n","  if tmp_score > best_score:\n","    best_score = tmp_score\n","    best_model = tmp_model\n","    best_comb = (epochs, lr, hidden_dim)\n","    print(f'new best score: {best_score}')\n","\n","print(f'[Grid Search] best combination (epochs, lr, hidden_dim): {best_comb}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-axDac7O8I_","executionInfo":{"status":"ok","timestamp":1695478661002,"user_tz":180,"elapsed":900204,"user":{"displayName":"Jose Cassano","userId":"06274683169262378241"}},"outputId":"bd79f6dd-97e2-4305-8c11-b05a73dd6498"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[(10, 0.1, 32), (10, 0.1, 64), (10, 0.1, 128), (10, 0.1, 256), (10, 0.01, 32), (10, 0.01, 64), (10, 0.01, 128), (10, 0.01, 256), (10, 0.001, 32), (10, 0.001, 64), (10, 0.001, 128), (10, 0.001, 256), (25, 0.1, 32), (25, 0.1, 64), (25, 0.1, 128), (25, 0.1, 256), (25, 0.01, 32), (25, 0.01, 64), (25, 0.01, 128), (25, 0.01, 256), (25, 0.001, 32), (25, 0.001, 64), (25, 0.001, 128), (25, 0.001, 256), (50, 0.1, 32), (50, 0.1, 64), (50, 0.1, 128), (50, 0.1, 256), (50, 0.01, 32), (50, 0.01, 64), (50, 0.01, 128), (50, 0.01, 256), (50, 0.001, 32), (50, 0.001, 64), (50, 0.001, 128), (50, 0.001, 256)]\n","Epoch [1/10] | Loss Train: 0.695545, Loss Test: 0.693177\n","Epoch [2/10] | Loss Train: 0.694005, Loss Test: 0.693933\n","Epoch [3/10] | Loss Train: 0.695325, Loss Test: 0.694950\n","Epoch [4/10] | Loss Train: 0.694756, Loss Test: 0.693849\n","Epoch [5/10] | Loss Train: 0.695280, Loss Test: 0.693904\n","Epoch [6/10] | Loss Train: 0.694796, Loss Test: 0.699982\n","Epoch [7/10] | Loss Train: 0.694902, Loss Test: 0.693667\n","Epoch [8/10] | Loss Train: 0.694437, Loss Test: 0.694675\n","Epoch [9/10] | Loss Train: 0.694565, Loss Test: 0.693163\n","Epoch [10/10] | Loss Train: 0.694603, Loss Test: 0.696906\n","[1/36] (Epochs: 10, Lr: 0.1, Hidden_dim: 32) =>\n","    best_model_score: 0 - train_score: 0.500801282051282\n"," new best score: 0.500801282051282\n","Epoch [1/10] | Loss Train: 0.419746, Loss Test: 0.357695\n","Epoch [2/10] | Loss Train: 0.361375, Loss Test: 0.347757\n","Epoch [3/10] | Loss Train: 0.356281, Loss Test: 0.355940\n","Epoch [4/10] | Loss Train: 0.352473, Loss Test: 0.336716\n","Epoch [5/10] | Loss Train: 0.343787, Loss Test: 0.370661\n","Epoch [6/10] | Loss Train: 0.344941, Loss Test: 0.341124\n","Epoch [7/10] | Loss Train: 0.342694, Loss Test: 0.345316\n","Epoch [8/10] | Loss Train: 0.343647, Loss Test: 0.339900\n","Epoch [9/10] | Loss Train: 0.338018, Loss Test: 0.331881\n","Epoch [10/10] | Loss Train: 0.336348, Loss Test: 0.334244\n","[2/36] (Epochs: 10, Lr: 0.1, Hidden_dim: 64) =>\n","    best_model_score: 0.500801282051282 - train_score: 0.8584535256410256\n"," new best score: 0.8584535256410256\n","Epoch [1/10] | Loss Train: 0.441152, Loss Test: 0.427399\n","Epoch [2/10] | Loss Train: 0.359822, Loss Test: 0.356285\n","Epoch [3/10] | Loss Train: 0.351755, Loss Test: 0.338261\n","Epoch [4/10] | Loss Train: 0.352909, Loss Test: 0.337920\n","Epoch [5/10] | Loss Train: 0.340248, Loss Test: 0.333172\n","Epoch [6/10] | Loss Train: 0.344724, Loss Test: 0.349037\n","Epoch [7/10] | Loss Train: 0.339017, Loss Test: 0.350596\n","Epoch [8/10] | Loss Train: 0.337337, Loss Test: 0.369173\n","Epoch [9/10] | Loss Train: 0.331840, Loss Test: 0.331827\n","Epoch [10/10] | Loss Train: 0.335556, Loss Test: 0.365702\n","[3/36] (Epochs: 10, Lr: 0.1, Hidden_dim: 128) =>\n","    best_model_score: 0.8584535256410256 - train_score: 0.8414663461538462\n","Epoch [1/10] | Loss Train: 0.443611, Loss Test: 0.355893\n","Epoch [2/10] | Loss Train: 0.362570, Loss Test: 0.372011\n","Epoch [3/10] | Loss Train: 0.357721, Loss Test: 0.374812\n","Epoch [4/10] | Loss Train: 0.355153, Loss Test: 0.338596\n","Epoch [5/10] | Loss Train: 0.347852, Loss Test: 0.340379\n","Epoch [6/10] | Loss Train: 0.350646, Loss Test: 0.348958\n","Epoch [7/10] | Loss Train: 0.345534, Loss Test: 0.347262\n","Epoch [8/10] | Loss Train: 0.347615, Loss Test: 0.334707\n","Epoch [9/10] | Loss Train: 0.343428, Loss Test: 0.343343\n","Epoch [10/10] | Loss Train: 0.338489, Loss Test: 0.331895\n","[4/36] (Epochs: 10, Lr: 0.1, Hidden_dim: 256) =>\n","    best_model_score: 0.8584535256410256 - train_score: 0.8614583333333333\n"," new best score: 0.8614583333333333\n","Epoch [1/10] | Loss Train: 0.425237, Loss Test: 0.357240\n","Epoch [2/10] | Loss Train: 0.357593, Loss Test: 0.344835\n","Epoch [3/10] | Loss Train: 0.350152, Loss Test: 0.400768\n","Epoch [4/10] | Loss Train: 0.348373, Loss Test: 0.338323\n","Epoch [5/10] | Loss Train: 0.340674, Loss Test: 0.364318\n","Epoch [6/10] | Loss Train: 0.339382, Loss Test: 0.336767\n","Epoch [7/10] | Loss Train: 0.334116, Loss Test: 0.339337\n","Epoch [8/10] | Loss Train: 0.335197, Loss Test: 0.333547\n","Epoch [9/10] | Loss Train: 0.330653, Loss Test: 0.352832\n","Epoch [10/10] | Loss Train: 0.330488, Loss Test: 0.339843\n","[5/36] (Epochs: 10, Lr: 0.01, Hidden_dim: 32) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.8536057692307693\n","Epoch [1/10] | Loss Train: 0.412505, Loss Test: 0.366066\n","Epoch [2/10] | Loss Train: 0.353675, Loss Test: 0.348765\n","Epoch [3/10] | Loss Train: 0.354358, Loss Test: 0.342193\n","Epoch [4/10] | Loss Train: 0.345061, Loss Test: 0.346032\n","Epoch [5/10] | Loss Train: 0.340842, Loss Test: 0.391575\n","Epoch [6/10] | Loss Train: 0.340645, Loss Test: 0.338369\n","Epoch [7/10] | Loss Train: 0.331234, Loss Test: 0.344118\n","Epoch [8/10] | Loss Train: 0.334250, Loss Test: 0.364422\n","Epoch [9/10] | Loss Train: 0.332574, Loss Test: 0.340915\n","Epoch [10/10] | Loss Train: 0.329136, Loss Test: 0.340119\n","[6/36] (Epochs: 10, Lr: 0.01, Hidden_dim: 64) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.8561298076923077\n","Epoch [1/10] | Loss Train: 0.406443, Loss Test: 0.527208\n","Epoch [2/10] | Loss Train: 0.363104, Loss Test: 0.349675\n","Epoch [3/10] | Loss Train: 0.348732, Loss Test: 0.346117\n","Epoch [4/10] | Loss Train: 0.341835, Loss Test: 0.335605\n","Epoch [5/10] | Loss Train: 0.337384, Loss Test: 0.353573\n","Epoch [6/10] | Loss Train: 0.332252, Loss Test: 0.329057\n","Epoch [7/10] | Loss Train: 0.335226, Loss Test: 0.346591\n","Epoch [8/10] | Loss Train: 0.334420, Loss Test: 0.335355\n","Epoch [9/10] | Loss Train: 0.330807, Loss Test: 0.331388\n","Epoch [10/10] | Loss Train: 0.325606, Loss Test: 0.354533\n","[7/36] (Epochs: 10, Lr: 0.01, Hidden_dim: 128) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.8441105769230769\n","Epoch [1/10] | Loss Train: 0.410097, Loss Test: 0.371838\n","Epoch [2/10] | Loss Train: 0.359334, Loss Test: 0.341847\n","Epoch [3/10] | Loss Train: 0.353534, Loss Test: 0.359699\n","Epoch [4/10] | Loss Train: 0.343046, Loss Test: 0.338350\n","Epoch [5/10] | Loss Train: 0.337019, Loss Test: 0.334901\n","Epoch [6/10] | Loss Train: 0.336808, Loss Test: 0.343728\n","Epoch [7/10] | Loss Train: 0.336266, Loss Test: 0.349468\n","Epoch [8/10] | Loss Train: 0.332089, Loss Test: 0.338070\n","Epoch [9/10] | Loss Train: 0.330771, Loss Test: 0.327848\n","Epoch [10/10] | Loss Train: 0.326394, Loss Test: 0.342228\n","[8/36] (Epochs: 10, Lr: 0.01, Hidden_dim: 256) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.8514022435897436\n","Epoch [1/10] | Loss Train: 0.626207, Loss Test: 0.529025\n","Epoch [2/10] | Loss Train: 0.463576, Loss Test: 0.423723\n","Epoch [3/10] | Loss Train: 0.400994, Loss Test: 0.389036\n","Epoch [4/10] | Loss Train: 0.377349, Loss Test: 0.373883\n","Epoch [5/10] | Loss Train: 0.365562, Loss Test: 0.370982\n","Epoch [6/10] | Loss Train: 0.358667, Loss Test: 0.359781\n","Epoch [7/10] | Loss Train: 0.353641, Loss Test: 0.353912\n","Epoch [8/10] | Loss Train: 0.350151, Loss Test: 0.351678\n","Epoch [9/10] | Loss Train: 0.346249, Loss Test: 0.348377\n","Epoch [10/10] | Loss Train: 0.343519, Loss Test: 0.348540\n","[9/36] (Epochs: 10, Lr: 0.001, Hidden_dim: 32) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.8504407051282051\n","Epoch [1/10] | Loss Train: 0.551806, Loss Test: 0.434014\n","Epoch [2/10] | Loss Train: 0.397897, Loss Test: 0.381972\n","Epoch [3/10] | Loss Train: 0.369432, Loss Test: 0.377783\n","Epoch [4/10] | Loss Train: 0.357583, Loss Test: 0.355761\n","Epoch [5/10] | Loss Train: 0.351789, Loss Test: 0.350173\n","Epoch [6/10] | Loss Train: 0.345991, Loss Test: 0.347605\n","Epoch [7/10] | Loss Train: 0.343754, Loss Test: 0.348452\n","Epoch [8/10] | Loss Train: 0.340250, Loss Test: 0.346757\n","Epoch [9/10] | Loss Train: 0.337734, Loss Test: 0.340004\n","Epoch [10/10] | Loss Train: 0.336103, Loss Test: 0.337969\n","[10/36] (Epochs: 10, Lr: 0.001, Hidden_dim: 64) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.8573717948717948\n","Epoch [1/10] | Loss Train: 0.519190, Loss Test: 0.406551\n","Epoch [2/10] | Loss Train: 0.382179, Loss Test: 0.369882\n","Epoch [3/10] | Loss Train: 0.361594, Loss Test: 0.370390\n","Epoch [4/10] | Loss Train: 0.351264, Loss Test: 0.348996\n","Epoch [5/10] | Loss Train: 0.346221, Loss Test: 0.348937\n","Epoch [6/10] | Loss Train: 0.341951, Loss Test: 0.346011\n","Epoch [7/10] | Loss Train: 0.338403, Loss Test: 0.338880\n","Epoch [8/10] | Loss Train: 0.337778, Loss Test: 0.339552\n","Epoch [9/10] | Loss Train: 0.334459, Loss Test: 0.335547\n","Epoch [10/10] | Loss Train: 0.333104, Loss Test: 0.334513\n","[11/36] (Epochs: 10, Lr: 0.001, Hidden_dim: 128) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.8593349358974359\n","Epoch [1/10] | Loss Train: 0.491673, Loss Test: 0.385317\n","Epoch [2/10] | Loss Train: 0.370368, Loss Test: 0.360869\n","Epoch [3/10] | Loss Train: 0.354030, Loss Test: 0.348695\n","Epoch [4/10] | Loss Train: 0.345691, Loss Test: 0.342915\n","Epoch [5/10] | Loss Train: 0.342904, Loss Test: 0.342364\n","Epoch [6/10] | Loss Train: 0.338613, Loss Test: 0.336357\n","Epoch [7/10] | Loss Train: 0.333649, Loss Test: 0.341762\n","Epoch [8/10] | Loss Train: 0.332252, Loss Test: 0.342215\n","Epoch [9/10] | Loss Train: 0.331809, Loss Test: 0.331820\n","Epoch [10/10] | Loss Train: 0.329078, Loss Test: 0.330964\n","[12/36] (Epochs: 10, Lr: 0.001, Hidden_dim: 256) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.8597756410256411\n","Epoch [1/25] | Loss Train: 0.697100, Loss Test: 0.698424\n","Epoch [2/25] | Loss Train: 0.694376, Loss Test: 0.693151\n","Epoch [3/25] | Loss Train: 0.694664, Loss Test: 0.695688\n","Epoch [4/25] | Loss Train: 0.694472, Loss Test: 0.693211\n","Epoch [5/25] | Loss Train: 0.693971, Loss Test: 0.695383\n","Epoch [6/25] | Loss Train: 0.694716, Loss Test: 0.693468\n","Epoch [7/25] | Loss Train: 0.694359, Loss Test: 0.693271\n","Epoch [8/25] | Loss Train: 0.694425, Loss Test: 0.693288\n","Epoch [9/25] | Loss Train: 0.694079, Loss Test: 0.693645\n","Epoch [10/25] | Loss Train: 0.695523, Loss Test: 0.694200\n","Epoch [11/25] | Loss Train: 0.694372, Loss Test: 0.693620\n","Epoch [12/25] | Loss Train: 0.694907, Loss Test: 0.694242\n","Epoch [13/25] | Loss Train: 0.695102, Loss Test: 0.693147\n","Epoch [14/25] | Loss Train: 0.694453, Loss Test: 0.693242\n","Epoch [15/25] | Loss Train: 0.695047, Loss Test: 0.695662\n","Epoch [16/25] | Loss Train: 0.694607, Loss Test: 0.696022\n","Epoch [17/25] | Loss Train: 0.694630, Loss Test: 0.694250\n","Epoch [18/25] | Loss Train: 0.694829, Loss Test: 0.694317\n","Epoch [19/25] | Loss Train: 0.694688, Loss Test: 0.693650\n","Epoch [20/25] | Loss Train: 0.694356, Loss Test: 0.693157\n","Epoch [21/25] | Loss Train: 0.694391, Loss Test: 0.693541\n","Epoch [22/25] | Loss Train: 0.694334, Loss Test: 0.695555\n","Epoch [23/25] | Loss Train: 0.694566, Loss Test: 0.693431\n","Epoch [24/25] | Loss Train: 0.694014, Loss Test: 0.695287\n","Epoch [25/25] | Loss Train: 0.694288, Loss Test: 0.694008\n","[13/36] (Epochs: 25, Lr: 0.1, Hidden_dim: 32) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.500801282051282\n","Epoch [1/25] | Loss Train: 0.436300, Loss Test: 0.358510\n","Epoch [2/25] | Loss Train: 0.357224, Loss Test: 0.346272\n","Epoch [3/25] | Loss Train: 0.356184, Loss Test: 0.349306\n","Epoch [4/25] | Loss Train: 0.353308, Loss Test: 0.369213\n","Epoch [5/25] | Loss Train: 0.348418, Loss Test: 0.348946\n","Epoch [6/25] | Loss Train: 0.344386, Loss Test: 0.385756\n","Epoch [7/25] | Loss Train: 0.341098, Loss Test: 0.346997\n","Epoch [8/25] | Loss Train: 0.340928, Loss Test: 0.339352\n","Epoch [9/25] | Loss Train: 0.340981, Loss Test: 0.340587\n","Epoch [10/25] | Loss Train: 0.339505, Loss Test: 0.372045\n","Epoch [11/25] | Loss Train: 0.340209, Loss Test: 0.357034\n","Epoch [12/25] | Loss Train: 0.342766, Loss Test: 0.337902\n","Epoch [13/25] | Loss Train: 0.341318, Loss Test: 0.360854\n","Epoch [14/25] | Loss Train: 0.338689, Loss Test: 0.338408\n","Epoch [15/25] | Loss Train: 0.339084, Loss Test: 0.333478\n","Epoch [16/25] | Loss Train: 0.333754, Loss Test: 0.333536\n","Epoch [17/25] | Loss Train: 0.339349, Loss Test: 0.345242\n","Epoch [18/25] | Loss Train: 0.333282, Loss Test: 0.339613\n","Epoch [19/25] | Loss Train: 0.338325, Loss Test: 0.335241\n","Epoch [20/25] | Loss Train: 0.338010, Loss Test: 0.334637\n","Epoch [21/25] | Loss Train: 0.333762, Loss Test: 0.336964\n","Epoch [22/25] | Loss Train: 0.340830, Loss Test: 0.345189\n","Epoch [23/25] | Loss Train: 0.332121, Loss Test: 0.358994\n","Epoch [24/25] | Loss Train: 0.333886, Loss Test: 0.334458\n","Epoch [25/25] | Loss Train: 0.335990, Loss Test: 0.354617\n","[14/36] (Epochs: 25, Lr: 0.1, Hidden_dim: 64) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.8507211538461539\n","Epoch [1/25] | Loss Train: 0.432409, Loss Test: 0.353149\n","Epoch [2/25] | Loss Train: 0.361282, Loss Test: 0.343968\n","Epoch [3/25] | Loss Train: 0.353223, Loss Test: 0.340343\n","Epoch [4/25] | Loss Train: 0.351929, Loss Test: 0.366454\n","Epoch [5/25] | Loss Train: 0.348195, Loss Test: 0.347324\n","Epoch [6/25] | Loss Train: 0.342638, Loss Test: 0.336643\n","Epoch [7/25] | Loss Train: 0.341825, Loss Test: 0.339491\n","Epoch [8/25] | Loss Train: 0.339494, Loss Test: 0.351839\n","Epoch [9/25] | Loss Train: 0.336163, Loss Test: 0.344551\n","Epoch [10/25] | Loss Train: 0.334329, Loss Test: 0.330869\n","Epoch [11/25] | Loss Train: 0.332831, Loss Test: 0.332035\n","Epoch [12/25] | Loss Train: 0.332353, Loss Test: 0.334106\n","Epoch [13/25] | Loss Train: 0.335405, Loss Test: 0.332189\n","Epoch [14/25] | Loss Train: 0.329153, Loss Test: 0.332483\n","Epoch [15/25] | Loss Train: 0.338612, Loss Test: 0.331532\n","Epoch [16/25] | Loss Train: 0.331920, Loss Test: 0.343321\n","Epoch [17/25] | Loss Train: 0.331123, Loss Test: 0.336800\n","Epoch [18/25] | Loss Train: 0.332558, Loss Test: 0.332885\n","Epoch [19/25] | Loss Train: 0.330184, Loss Test: 0.350565\n","Epoch [20/25] | Loss Train: 0.326992, Loss Test: 0.395984\n","Epoch [21/25] | Loss Train: 0.329736, Loss Test: 0.348813\n","Epoch [22/25] | Loss Train: 0.330536, Loss Test: 0.344081\n","Epoch [23/25] | Loss Train: 0.328296, Loss Test: 0.353000\n","Epoch [24/25] | Loss Train: 0.329293, Loss Test: 0.331141\n","Epoch [25/25] | Loss Train: 0.332552, Loss Test: 0.343055\n","[15/36] (Epochs: 25, Lr: 0.1, Hidden_dim: 128) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.8526041666666667\n","Epoch [1/25] | Loss Train: 0.460756, Loss Test: 0.390960\n","Epoch [2/25] | Loss Train: 0.363099, Loss Test: 0.356653\n","Epoch [3/25] | Loss Train: 0.356684, Loss Test: 0.344664\n","Epoch [4/25] | Loss Train: 0.346924, Loss Test: 0.357234\n","Epoch [5/25] | Loss Train: 0.347048, Loss Test: 0.343064\n","Epoch [6/25] | Loss Train: 0.342555, Loss Test: 0.340747\n","Epoch [7/25] | Loss Train: 0.337961, Loss Test: 0.337491\n","Epoch [8/25] | Loss Train: 0.338892, Loss Test: 0.333927\n","Epoch [9/25] | Loss Train: 0.338602, Loss Test: 0.348041\n","Epoch [10/25] | Loss Train: 0.335923, Loss Test: 0.335175\n","Epoch [11/25] | Loss Train: 0.336504, Loss Test: 0.366073\n","Epoch [12/25] | Loss Train: 0.337703, Loss Test: 0.353196\n","Epoch [13/25] | Loss Train: 0.337208, Loss Test: 0.352588\n","Epoch [14/25] | Loss Train: 0.335374, Loss Test: 0.367107\n","Epoch [15/25] | Loss Train: 0.335718, Loss Test: 0.375445\n","Epoch [16/25] | Loss Train: 0.337435, Loss Test: 0.337308\n","Epoch [17/25] | Loss Train: 0.334078, Loss Test: 0.334564\n","Epoch [18/25] | Loss Train: 0.334829, Loss Test: 0.331787\n","Epoch [19/25] | Loss Train: 0.327369, Loss Test: 0.370509\n","Epoch [20/25] | Loss Train: 0.331989, Loss Test: 0.336038\n","Epoch [21/25] | Loss Train: 0.335942, Loss Test: 0.331758\n","Epoch [22/25] | Loss Train: 0.328817, Loss Test: 0.392855\n","Epoch [23/25] | Loss Train: 0.328490, Loss Test: 0.333689\n","Epoch [24/25] | Loss Train: 0.328384, Loss Test: 0.347185\n","Epoch [25/25] | Loss Train: 0.328491, Loss Test: 0.347366\n","[16/36] (Epochs: 25, Lr: 0.1, Hidden_dim: 256) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.8516426282051283\n","Epoch [1/25] | Loss Train: 0.442733, Loss Test: 0.370993\n","Epoch [2/25] | Loss Train: 0.360172, Loss Test: 0.351239\n","Epoch [3/25] | Loss Train: 0.353946, Loss Test: 0.342075\n","Epoch [4/25] | Loss Train: 0.346005, Loss Test: 0.342194\n","Epoch [5/25] | Loss Train: 0.340683, Loss Test: 0.346460\n","Epoch [6/25] | Loss Train: 0.338297, Loss Test: 0.501930\n","Epoch [7/25] | Loss Train: 0.339810, Loss Test: 0.405076\n","Epoch [8/25] | Loss Train: 0.339058, Loss Test: 0.336361\n","Epoch [9/25] | Loss Train: 0.333941, Loss Test: 0.337356\n","Epoch [10/25] | Loss Train: 0.335011, Loss Test: 0.358564\n","Epoch [11/25] | Loss Train: 0.331054, Loss Test: 0.344916\n","Epoch [12/25] | Loss Train: 0.329061, Loss Test: 0.333204\n","Epoch [13/25] | Loss Train: 0.331238, Loss Test: 0.357407\n","Epoch [14/25] | Loss Train: 0.329169, Loss Test: 0.330235\n","Epoch [15/25] | Loss Train: 0.330754, Loss Test: 0.333688\n","Epoch [16/25] | Loss Train: 0.327810, Loss Test: 0.328456\n","Epoch [17/25] | Loss Train: 0.329041, Loss Test: 0.329690\n","Epoch [18/25] | Loss Train: 0.326957, Loss Test: 0.328006\n","Epoch [19/25] | Loss Train: 0.322531, Loss Test: 0.334186\n","Epoch [20/25] | Loss Train: 0.325031, Loss Test: 0.329293\n","Epoch [21/25] | Loss Train: 0.323711, Loss Test: 0.327456\n","Epoch [22/25] | Loss Train: 0.323499, Loss Test: 0.326940\n","Epoch [23/25] | Loss Train: 0.319437, Loss Test: 0.355167\n","Epoch [24/25] | Loss Train: 0.319276, Loss Test: 0.327580\n","Epoch [25/25] | Loss Train: 0.320485, Loss Test: 0.330195\n","[17/36] (Epochs: 25, Lr: 0.01, Hidden_dim: 32) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.8590544871794872\n","Epoch [1/25] | Loss Train: 0.424204, Loss Test: 0.356672\n","Epoch [2/25] | Loss Train: 0.358198, Loss Test: 0.345964\n","Epoch [3/25] | Loss Train: 0.348808, Loss Test: 0.345449\n","Epoch [4/25] | Loss Train: 0.347161, Loss Test: 0.337142\n","Epoch [5/25] | Loss Train: 0.341391, Loss Test: 0.422344\n","Epoch [6/25] | Loss Train: 0.337638, Loss Test: 0.341626\n","Epoch [7/25] | Loss Train: 0.338913, Loss Test: 0.338601\n","Epoch [8/25] | Loss Train: 0.330964, Loss Test: 0.328753\n","Epoch [9/25] | Loss Train: 0.329733, Loss Test: 0.373435\n","Epoch [10/25] | Loss Train: 0.328850, Loss Test: 0.347833\n","Epoch [11/25] | Loss Train: 0.327112, Loss Test: 0.325976\n","Epoch [12/25] | Loss Train: 0.323670, Loss Test: 0.331423\n","Epoch [13/25] | Loss Train: 0.323127, Loss Test: 0.336886\n","Epoch [14/25] | Loss Train: 0.320896, Loss Test: 0.327397\n","Epoch [15/25] | Loss Train: 0.320564, Loss Test: 0.328557\n","Epoch [16/25] | Loss Train: 0.315681, Loss Test: 0.329067\n","Epoch [17/25] | Loss Train: 0.315666, Loss Test: 0.376933\n","Epoch [18/25] | Loss Train: 0.314515, Loss Test: 0.325107\n","Epoch [19/25] | Loss Train: 0.314981, Loss Test: 0.336713\n","Epoch [20/25] | Loss Train: 0.311144, Loss Test: 0.325226\n","Epoch [21/25] | Loss Train: 0.308962, Loss Test: 0.329422\n","Epoch [22/25] | Loss Train: 0.309409, Loss Test: 0.343354\n","Epoch [23/25] | Loss Train: 0.307921, Loss Test: 0.331691\n","Epoch [24/25] | Loss Train: 0.304118, Loss Test: 0.343533\n","Epoch [25/25] | Loss Train: 0.303520, Loss Test: 0.328721\n","[18/36] (Epochs: 25, Lr: 0.01, Hidden_dim: 64) =>\n","    best_model_score: 0.8614583333333333 - train_score: 0.862139423076923\n"," new best score: 0.862139423076923\n","Epoch [1/25] | Loss Train: 0.429327, Loss Test: 0.355992\n","Epoch [2/25] | Loss Train: 0.355479, Loss Test: 0.356417\n","Epoch [3/25] | Loss Train: 0.348700, Loss Test: 0.376283\n","Epoch [4/25] | Loss Train: 0.346493, Loss Test: 0.347858\n","Epoch [5/25] | Loss Train: 0.342371, Loss Test: 0.346265\n","Epoch [6/25] | Loss Train: 0.335540, Loss Test: 0.332271\n","Epoch [7/25] | Loss Train: 0.334450, Loss Test: 0.396556\n","Epoch [8/25] | Loss Train: 0.333544, Loss Test: 0.329688\n","Epoch [9/25] | Loss Train: 0.329811, Loss Test: 0.332791\n","Epoch [10/25] | Loss Train: 0.325624, Loss Test: 0.331012\n","Epoch [11/25] | Loss Train: 0.325930, Loss Test: 0.329609\n","Epoch [12/25] | Loss Train: 0.322841, Loss Test: 0.326301\n","Epoch [13/25] | Loss Train: 0.324326, Loss Test: 0.366020\n","Epoch [14/25] | Loss Train: 0.319256, Loss Test: 0.330160\n","Epoch [15/25] | Loss Train: 0.317960, Loss Test: 0.332348\n","Epoch [16/25] | Loss Train: 0.317260, Loss Test: 0.324227\n","Epoch [17/25] | Loss Train: 0.313975, Loss Test: 0.332319\n","Epoch [18/25] | Loss Train: 0.312814, Loss Test: 0.325472\n","Epoch [19/25] | Loss Train: 0.312983, Loss Test: 0.325840\n","Epoch [20/25] | Loss Train: 0.316440, Loss Test: 0.325281\n","Epoch [21/25] | Loss Train: 0.310590, Loss Test: 0.331827\n","Epoch [22/25] | Loss Train: 0.307797, Loss Test: 0.334056\n","Epoch [23/25] | Loss Train: 0.307713, Loss Test: 0.322442\n","Epoch [24/25] | Loss Train: 0.307479, Loss Test: 0.327932\n","Epoch [25/25] | Loss Train: 0.302926, Loss Test: 0.326608\n","[19/36] (Epochs: 25, Lr: 0.01, Hidden_dim: 128) =>\n","    best_model_score: 0.862139423076923 - train_score: 0.8626201923076923\n"," new best score: 0.8626201923076923\n","Epoch [1/25] | Loss Train: 0.434766, Loss Test: 0.354438\n","Epoch [2/25] | Loss Train: 0.355793, Loss Test: 0.344663\n","Epoch [3/25] | Loss Train: 0.345950, Loss Test: 0.355854\n","Epoch [4/25] | Loss Train: 0.346235, Loss Test: 0.350261\n","Epoch [5/25] | Loss Train: 0.334267, Loss Test: 0.334304\n","Epoch [6/25] | Loss Train: 0.336700, Loss Test: 0.333181\n","Epoch [7/25] | Loss Train: 0.331239, Loss Test: 0.339960\n","Epoch [8/25] | Loss Train: 0.328294, Loss Test: 0.362204\n","Epoch [9/25] | Loss Train: 0.328395, Loss Test: 0.328804\n","Epoch [10/25] | Loss Train: 0.328406, Loss Test: 0.351744\n","Epoch [11/25] | Loss Train: 0.322858, Loss Test: 0.353127\n","Epoch [12/25] | Loss Train: 0.326016, Loss Test: 0.326935\n","Epoch [13/25] | Loss Train: 0.321838, Loss Test: 0.331552\n","Epoch [14/25] | Loss Train: 0.320895, Loss Test: 0.333252\n","Epoch [15/25] | Loss Train: 0.318963, Loss Test: 0.362186\n","Epoch [16/25] | Loss Train: 0.315249, Loss Test: 0.332586\n","Epoch [17/25] | Loss Train: 0.317007, Loss Test: 0.325511\n","Epoch [18/25] | Loss Train: 0.313393, Loss Test: 0.324430\n","Epoch [19/25] | Loss Train: 0.313488, Loss Test: 0.328626\n","Epoch [20/25] | Loss Train: 0.312878, Loss Test: 0.342197\n","Epoch [21/25] | Loss Train: 0.308542, Loss Test: 0.338900\n","Epoch [22/25] | Loss Train: 0.311482, Loss Test: 0.326557\n","Epoch [23/25] | Loss Train: 0.310475, Loss Test: 0.337760\n","Epoch [24/25] | Loss Train: 0.306941, Loss Test: 0.331877\n","Epoch [25/25] | Loss Train: 0.304955, Loss Test: 0.327509\n","[20/36] (Epochs: 25, Lr: 0.01, Hidden_dim: 256) =>\n","    best_model_score: 0.8626201923076923 - train_score: 0.8604567307692308\n","Epoch [1/25] | Loss Train: 0.603041, Loss Test: 0.490287\n","Epoch [2/25] | Loss Train: 0.433819, Loss Test: 0.403449\n","Epoch [3/25] | Loss Train: 0.387652, Loss Test: 0.388663\n","Epoch [4/25] | Loss Train: 0.370296, Loss Test: 0.368807\n","Epoch [5/25] | Loss Train: 0.360979, Loss Test: 0.360650\n","Epoch [6/25] | Loss Train: 0.354034, Loss Test: 0.354469\n","Epoch [7/25] | Loss Train: 0.350827, Loss Test: 0.352192\n","Epoch [8/25] | Loss Train: 0.346607, Loss Test: 0.349041\n","Epoch [9/25] | Loss Train: 0.344749, Loss Test: 0.346085\n","Epoch [10/25] | Loss Train: 0.342990, Loss Test: 0.357342\n","Epoch [11/25] | Loss Train: 0.340572, Loss Test: 0.342371\n","Epoch [12/25] | Loss Train: 0.338126, Loss Test: 0.341505\n","Epoch [13/25] | Loss Train: 0.336500, Loss Test: 0.343612\n","Epoch [14/25] | Loss Train: 0.335400, Loss Test: 0.339530\n","Epoch [15/25] | Loss Train: 0.334917, Loss Test: 0.337856\n","Epoch [16/25] | Loss Train: 0.332486, Loss Test: 0.338228\n","Epoch [17/25] | Loss Train: 0.332474, Loss Test: 0.336057\n","Epoch [18/25] | Loss Train: 0.331200, Loss Test: 0.337114\n","Epoch [19/25] | Loss Train: 0.330200, Loss Test: 0.335086\n","Epoch [20/25] | Loss Train: 0.330168, Loss Test: 0.344941\n","Epoch [21/25] | Loss Train: 0.327687, Loss Test: 0.333379\n","Epoch [22/25] | Loss Train: 0.328231, Loss Test: 0.333159\n","Epoch [23/25] | Loss Train: 0.327074, Loss Test: 0.337053\n","Epoch [24/25] | Loss Train: 0.326409, Loss Test: 0.331529\n","Epoch [25/25] | Loss Train: 0.325229, Loss Test: 0.333864\n","[21/36] (Epochs: 25, Lr: 0.001, Hidden_dim: 32) =>\n","    best_model_score: 0.8626201923076923 - train_score: 0.8579326923076923\n","Epoch [1/25] | Loss Train: 0.557170, Loss Test: 0.433741\n","Epoch [2/25] | Loss Train: 0.398032, Loss Test: 0.380579\n","Epoch [3/25] | Loss Train: 0.369989, Loss Test: 0.363756\n","Epoch [4/25] | Loss Train: 0.358027, Loss Test: 0.356268\n","Epoch [5/25] | Loss Train: 0.349966, Loss Test: 0.350449\n","Epoch [6/25] | Loss Train: 0.346082, Loss Test: 0.349731\n","Epoch [7/25] | Loss Train: 0.342133, Loss Test: 0.357066\n","Epoch [8/25] | Loss Train: 0.339348, Loss Test: 0.341970\n","Epoch [9/25] | Loss Train: 0.338602, Loss Test: 0.348053\n","Epoch [10/25] | Loss Train: 0.335485, Loss Test: 0.339015\n","Epoch [11/25] | Loss Train: 0.334012, Loss Test: 0.337588\n","Epoch [12/25] | Loss Train: 0.332970, Loss Test: 0.334852\n","Epoch [13/25] | Loss Train: 0.330544, Loss Test: 0.334739\n","Epoch [14/25] | Loss Train: 0.328813, Loss Test: 0.333015\n","Epoch [15/25] | Loss Train: 0.327757, Loss Test: 0.332053\n","Epoch [16/25] | Loss Train: 0.326297, Loss Test: 0.331588\n","Epoch [17/25] | Loss Train: 0.327775, Loss Test: 0.332606\n","Epoch [18/25] | Loss Train: 0.326141, Loss Test: 0.334452\n","Epoch [19/25] | Loss Train: 0.324279, Loss Test: 0.329400\n","Epoch [20/25] | Loss Train: 0.324428, Loss Test: 0.329282\n","Epoch [21/25] | Loss Train: 0.322516, Loss Test: 0.328366\n","Epoch [22/25] | Loss Train: 0.322653, Loss Test: 0.330978\n","Epoch [23/25] | Loss Train: 0.320371, Loss Test: 0.333865\n","Epoch [24/25] | Loss Train: 0.320625, Loss Test: 0.329377\n","Epoch [25/25] | Loss Train: 0.319234, Loss Test: 0.332296\n","[22/36] (Epochs: 25, Lr: 0.001, Hidden_dim: 64) =>\n","    best_model_score: 0.8626201923076923 - train_score: 0.8584935897435897\n","Epoch [1/25] | Loss Train: 0.539188, Loss Test: 0.412757\n","Epoch [2/25] | Loss Train: 0.386811, Loss Test: 0.373676\n","Epoch [3/25] | Loss Train: 0.363356, Loss Test: 0.357916\n","Epoch [4/25] | Loss Train: 0.354226, Loss Test: 0.356080\n","Epoch [5/25] | Loss Train: 0.349870, Loss Test: 0.346763\n","Epoch [6/25] | Loss Train: 0.344619, Loss Test: 0.343976\n","Epoch [7/25] | Loss Train: 0.341575, Loss Test: 0.341340\n","Epoch [8/25] | Loss Train: 0.338836, Loss Test: 0.377755\n","Epoch [9/25] | Loss Train: 0.337594, Loss Test: 0.338270\n","Epoch [10/25] | Loss Train: 0.336669, Loss Test: 0.341606\n","Epoch [11/25] | Loss Train: 0.334015, Loss Test: 0.341059\n","Epoch [12/25] | Loss Train: 0.331972, Loss Test: 0.335464\n","Epoch [13/25] | Loss Train: 0.331671, Loss Test: 0.337340\n","Epoch [14/25] | Loss Train: 0.330530, Loss Test: 0.334760\n","Epoch [15/25] | Loss Train: 0.329587, Loss Test: 0.336748\n","Epoch [16/25] | Loss Train: 0.329241, Loss Test: 0.337984\n","Epoch [17/25] | Loss Train: 0.327703, Loss Test: 0.332271\n","Epoch [18/25] | Loss Train: 0.326718, Loss Test: 0.331690\n","Epoch [19/25] | Loss Train: 0.326924, Loss Test: 0.349086\n","Epoch [20/25] | Loss Train: 0.325852, Loss Test: 0.342391\n","Epoch [21/25] | Loss Train: 0.326264, Loss Test: 0.332121\n","Epoch [22/25] | Loss Train: 0.323782, Loss Test: 0.332138\n","Epoch [23/25] | Loss Train: 0.324735, Loss Test: 0.338023\n","Epoch [24/25] | Loss Train: 0.323162, Loss Test: 0.332810\n","Epoch [25/25] | Loss Train: 0.322793, Loss Test: 0.336470\n","[23/36] (Epochs: 25, Lr: 0.001, Hidden_dim: 128) =>\n","    best_model_score: 0.8626201923076923 - train_score: 0.8577724358974359\n","Epoch [1/25] | Loss Train: 0.496556, Loss Test: 0.389114\n","Epoch [2/25] | Loss Train: 0.374110, Loss Test: 0.365428\n","Epoch [3/25] | Loss Train: 0.354661, Loss Test: 0.349912\n","Epoch [4/25] | Loss Train: 0.348555, Loss Test: 0.344242\n","Epoch [5/25] | Loss Train: 0.342992, Loss Test: 0.342660\n","Epoch [6/25] | Loss Train: 0.337830, Loss Test: 0.341094\n","Epoch [7/25] | Loss Train: 0.335833, Loss Test: 0.336270\n","Epoch [8/25] | Loss Train: 0.332590, Loss Test: 0.334209\n","Epoch [9/25] | Loss Train: 0.331375, Loss Test: 0.334369\n","Epoch [10/25] | Loss Train: 0.327579, Loss Test: 0.333379\n","Epoch [11/25] | Loss Train: 0.328426, Loss Test: 0.331114\n","Epoch [12/25] | Loss Train: 0.326685, Loss Test: 0.337551\n","Epoch [13/25] | Loss Train: 0.324591, Loss Test: 0.330184\n","Epoch [14/25] | Loss Train: 0.323602, Loss Test: 0.329738\n","Epoch [15/25] | Loss Train: 0.324235, Loss Test: 0.328774\n","Epoch [16/25] | Loss Train: 0.320708, Loss Test: 0.333449\n","Epoch [17/25] | Loss Train: 0.319504, Loss Test: 0.327558\n","Epoch [18/25] | Loss Train: 0.318857, Loss Test: 0.325372\n","Epoch [19/25] | Loss Train: 0.317382, Loss Test: 0.335695\n","Epoch [20/25] | Loss Train: 0.316032, Loss Test: 0.348554\n","Epoch [21/25] | Loss Train: 0.315579, Loss Test: 0.327062\n","Epoch [22/25] | Loss Train: 0.314430, Loss Test: 0.324009\n","Epoch [23/25] | Loss Train: 0.313782, Loss Test: 0.335382\n","Epoch [24/25] | Loss Train: 0.310901, Loss Test: 0.326857\n","Epoch [25/25] | Loss Train: 0.313316, Loss Test: 0.329140\n","[24/36] (Epochs: 25, Lr: 0.001, Hidden_dim: 256) =>\n","    best_model_score: 0.8626201923076923 - train_score: 0.8585336538461539\n","Epoch [1/50] | Loss Train: 0.696043, Loss Test: 0.693601\n","Epoch [2/50] | Loss Train: 0.694038, Loss Test: 0.697026\n","Epoch [3/50] | Loss Train: 0.694784, Loss Test: 0.693227\n","Epoch [4/50] | Loss Train: 0.694342, Loss Test: 0.694842\n","Epoch [5/50] | Loss Train: 0.694495, Loss Test: 0.693185\n","Epoch [6/50] | Loss Train: 0.694863, Loss Test: 0.696252\n","Epoch [7/50] | Loss Train: 0.694875, Loss Test: 0.693489\n","Epoch [8/50] | Loss Train: 0.694821, Loss Test: 0.693797\n","Epoch [9/50] | Loss Train: 0.694538, Loss Test: 0.693324\n","Epoch [10/50] | Loss Train: 0.694561, Loss Test: 0.697722\n","Epoch [11/50] | Loss Train: 0.694129, Loss Test: 0.694082\n","Epoch [12/50] | Loss Train: 0.694447, Loss Test: 0.693835\n","Epoch [13/50] | Loss Train: 0.694442, Loss Test: 0.694044\n","Epoch [14/50] | Loss Train: 0.694509, Loss Test: 0.693228\n","Epoch [15/50] | Loss Train: 0.694475, Loss Test: 0.694666\n","Epoch [16/50] | Loss Train: 0.694209, Loss Test: 0.693239\n","Epoch [17/50] | Loss Train: 0.694248, Loss Test: 0.698283\n","Epoch [18/50] | Loss Train: 0.694316, Loss Test: 0.694259\n","Epoch [19/50] | Loss Train: 0.695030, Loss Test: 0.694227\n","Epoch [20/50] | Loss Train: 0.694984, Loss Test: 0.693147\n","Epoch [21/50] | Loss Train: 0.694204, Loss Test: 0.693338\n","Epoch [22/50] | Loss Train: 0.694736, Loss Test: 0.697895\n","Epoch [23/50] | Loss Train: 0.695334, Loss Test: 0.693378\n","Epoch [24/50] | Loss Train: 0.694686, Loss Test: 0.693445\n","Epoch [25/50] | Loss Train: 0.694046, Loss Test: 0.693987\n","Epoch [26/50] | Loss Train: 0.694511, Loss Test: 0.699007\n","Epoch [27/50] | Loss Train: 0.694317, Loss Test: 0.694152\n","Epoch [28/50] | Loss Train: 0.694404, Loss Test: 0.693237\n","Epoch [29/50] | Loss Train: 0.693974, Loss Test: 0.696944\n","Epoch [30/50] | Loss Train: 0.694567, Loss Test: 0.698126\n","Epoch [31/50] | Loss Train: 0.695075, Loss Test: 0.693753\n","Epoch [32/50] | Loss Train: 0.694305, Loss Test: 0.693627\n","Epoch [33/50] | Loss Train: 0.693963, Loss Test: 0.693890\n","Epoch [34/50] | Loss Train: 0.695241, Loss Test: 0.693498\n","Epoch [35/50] | Loss Train: 0.695387, Loss Test: 0.699174\n","Epoch [36/50] | Loss Train: 0.694527, Loss Test: 0.693703\n","Epoch [37/50] | Loss Train: 0.694314, Loss Test: 0.696270\n","Epoch [38/50] | Loss Train: 0.695308, Loss Test: 0.695590\n","Epoch [39/50] | Loss Train: 0.694867, Loss Test: 0.693485\n","Epoch [40/50] | Loss Train: 0.693990, Loss Test: 0.693146\n","Epoch [41/50] | Loss Train: 0.694253, Loss Test: 0.693272\n","Epoch [42/50] | Loss Train: 0.694137, Loss Test: 0.694125\n","Epoch [43/50] | Loss Train: 0.695578, Loss Test: 0.699133\n","Epoch [44/50] | Loss Train: 0.694547, Loss Test: 0.694470\n","Epoch [45/50] | Loss Train: 0.694626, Loss Test: 0.693819\n","Epoch [46/50] | Loss Train: 0.694440, Loss Test: 0.699704\n","Epoch [47/50] | Loss Train: 0.694956, Loss Test: 0.695833\n","Epoch [48/50] | Loss Train: 0.695249, Loss Test: 0.695631\n","Epoch [49/50] | Loss Train: 0.694035, Loss Test: 0.699515\n","Epoch [50/50] | Loss Train: 0.694697, Loss Test: 0.694347\n","[25/36] (Epochs: 50, Lr: 0.1, Hidden_dim: 32) =>\n","    best_model_score: 0.8626201923076923 - train_score: 0.49919871794871795\n","Epoch [1/50] | Loss Train: 0.414647, Loss Test: 0.356308\n","Epoch [2/50] | Loss Train: 0.384327, Loss Test: 0.364228\n","Epoch [3/50] | Loss Train: 0.355561, Loss Test: 0.385091\n","Epoch [4/50] | Loss Train: 0.353220, Loss Test: 0.338646\n","Epoch [5/50] | Loss Train: 0.352249, Loss Test: 0.357385\n","Epoch [6/50] | Loss Train: 0.347071, Loss Test: 0.333957\n","Epoch [7/50] | Loss Train: 0.340481, Loss Test: 0.343498\n","Epoch [8/50] | Loss Train: 0.340544, Loss Test: 0.334429\n","Epoch [9/50] | Loss Train: 0.341147, Loss Test: 0.367547\n","Epoch [10/50] | Loss Train: 0.340151, Loss Test: 0.347726\n","Epoch [11/50] | Loss Train: 0.336105, Loss Test: 0.332848\n","Epoch [12/50] | Loss Train: 0.336207, Loss Test: 0.345002\n","Epoch [13/50] | Loss Train: 0.334220, Loss Test: 0.411849\n","Epoch [14/50] | Loss Train: 0.337284, Loss Test: 0.339090\n","Epoch [15/50] | Loss Train: 0.329505, Loss Test: 0.339833\n","Epoch [16/50] | Loss Train: 0.330896, Loss Test: 0.369284\n","Epoch [17/50] | Loss Train: 0.329663, Loss Test: 0.338445\n","Epoch [18/50] | Loss Train: 0.325796, Loss Test: 0.339061\n","Epoch [19/50] | Loss Train: 0.328341, Loss Test: 0.329229\n","Epoch [20/50] | Loss Train: 0.332694, Loss Test: 0.333301\n","Epoch [21/50] | Loss Train: 0.330029, Loss Test: 0.346551\n","Epoch [22/50] | Loss Train: 0.327138, Loss Test: 0.350588\n","Epoch [23/50] | Loss Train: 0.329722, Loss Test: 0.361500\n","Epoch [24/50] | Loss Train: 0.330656, Loss Test: 0.339513\n","Epoch [25/50] | Loss Train: 0.332005, Loss Test: 0.339748\n","Epoch [26/50] | Loss Train: 0.329722, Loss Test: 0.331946\n","Epoch [27/50] | Loss Train: 0.322780, Loss Test: 0.333518\n","Epoch [28/50] | Loss Train: 0.330621, Loss Test: 0.336493\n","Epoch [29/50] | Loss Train: 0.327457, Loss Test: 0.343314\n","Epoch [30/50] | Loss Train: 0.323820, Loss Test: 0.332956\n","Epoch [31/50] | Loss Train: 0.325045, Loss Test: 0.366601\n","Epoch [32/50] | Loss Train: 0.327854, Loss Test: 0.336516\n","Epoch [33/50] | Loss Train: 0.321792, Loss Test: 0.332871\n","Epoch [34/50] | Loss Train: 0.323293, Loss Test: 0.340944\n","Epoch [35/50] | Loss Train: 0.322497, Loss Test: 0.350816\n","Epoch [36/50] | Loss Train: 0.321710, Loss Test: 0.334557\n","Epoch [37/50] | Loss Train: 0.325225, Loss Test: 0.339577\n","Epoch [38/50] | Loss Train: 0.322737, Loss Test: 0.350307\n","Epoch [39/50] | Loss Train: 0.319700, Loss Test: 0.338823\n","Epoch [40/50] | Loss Train: 0.322761, Loss Test: 0.365437\n","Epoch [41/50] | Loss Train: 0.336957, Loss Test: 0.336847\n","Epoch [42/50] | Loss Train: 0.325246, Loss Test: 0.365993\n","Epoch [43/50] | Loss Train: 0.329319, Loss Test: 0.354645\n","Epoch [44/50] | Loss Train: 0.320064, Loss Test: 0.364449\n","Epoch [45/50] | Loss Train: 0.322682, Loss Test: 0.348075\n","Epoch [46/50] | Loss Train: 0.320993, Loss Test: 0.340297\n","Epoch [47/50] | Loss Train: 0.321873, Loss Test: 0.339049\n","Epoch [48/50] | Loss Train: 0.321276, Loss Test: 0.331186\n","Epoch [49/50] | Loss Train: 0.318192, Loss Test: 0.332311\n","Epoch [50/50] | Loss Train: 0.319948, Loss Test: 0.333150\n","[26/36] (Epochs: 50, Lr: 0.1, Hidden_dim: 64) =>\n","    best_model_score: 0.8626201923076923 - train_score: 0.8585336538461539\n","Epoch [1/50] | Loss Train: 0.429725, Loss Test: 0.374099\n","Epoch [2/50] | Loss Train: 0.371390, Loss Test: 0.343573\n","Epoch [3/50] | Loss Train: 0.354297, Loss Test: 0.342644\n","Epoch [4/50] | Loss Train: 0.345518, Loss Test: 0.355590\n","Epoch [5/50] | Loss Train: 0.345906, Loss Test: 0.387692\n","Epoch [6/50] | Loss Train: 0.345642, Loss Test: 0.407584\n","Epoch [7/50] | Loss Train: 0.342622, Loss Test: 0.343142\n","Epoch [8/50] | Loss Train: 0.341575, Loss Test: 0.348980\n","Epoch [9/50] | Loss Train: 0.338099, Loss Test: 0.335837\n","Epoch [10/50] | Loss Train: 0.331058, Loss Test: 0.350141\n","Epoch [11/50] | Loss Train: 0.334826, Loss Test: 0.338641\n","Epoch [12/50] | Loss Train: 0.332467, Loss Test: 0.339605\n","Epoch [13/50] | Loss Train: 0.337130, Loss Test: 0.342996\n","Epoch [14/50] | Loss Train: 0.328778, Loss Test: 0.334076\n","Epoch [15/50] | Loss Train: 0.330553, Loss Test: 0.328364\n","Epoch [16/50] | Loss Train: 0.331102, Loss Test: 0.341853\n","Epoch [17/50] | Loss Train: 0.326556, Loss Test: 0.338838\n","Epoch [18/50] | Loss Train: 0.330135, Loss Test: 0.340830\n","Epoch [19/50] | Loss Train: 0.327780, Loss Test: 0.339483\n","Epoch [20/50] | Loss Train: 0.324657, Loss Test: 0.340774\n","Epoch [21/50] | Loss Train: 0.325559, Loss Test: 0.334623\n","Epoch [22/50] | Loss Train: 0.331177, Loss Test: 0.337167\n","Epoch [23/50] | Loss Train: 0.324303, Loss Test: 0.353624\n","Epoch [24/50] | Loss Train: 0.325929, Loss Test: 0.348352\n","Epoch [25/50] | Loss Train: 0.322875, Loss Test: 0.331615\n","Epoch [26/50] | Loss Train: 0.324560, Loss Test: 0.345576\n","Epoch [27/50] | Loss Train: 0.320646, Loss Test: 0.368227\n","Epoch [28/50] | Loss Train: 0.326842, Loss Test: 0.348458\n","Epoch [29/50] | Loss Train: 0.323847, Loss Test: 0.349024\n","Epoch [30/50] | Loss Train: 0.319560, Loss Test: 0.328871\n","Epoch [31/50] | Loss Train: 0.316242, Loss Test: 0.354711\n","Epoch [32/50] | Loss Train: 0.320649, Loss Test: 0.335734\n","Epoch [33/50] | Loss Train: 0.319453, Loss Test: 0.338165\n","Epoch [34/50] | Loss Train: 0.320806, Loss Test: 0.341135\n","Epoch [35/50] | Loss Train: 0.320028, Loss Test: 0.336353\n","Epoch [36/50] | Loss Train: 0.322994, Loss Test: 0.332925\n","Epoch [37/50] | Loss Train: 0.317275, Loss Test: 0.337716\n","Epoch [38/50] | Loss Train: 0.325103, Loss Test: 0.333752\n","Epoch [39/50] | Loss Train: 0.316695, Loss Test: 0.339624\n","Epoch [40/50] | Loss Train: 0.325066, Loss Test: 0.331068\n","Epoch [41/50] | Loss Train: 0.321272, Loss Test: 0.346921\n","Epoch [42/50] | Loss Train: 0.319547, Loss Test: 0.344465\n","Epoch [43/50] | Loss Train: 0.320460, Loss Test: 0.339007\n","Epoch [44/50] | Loss Train: 0.314954, Loss Test: 0.352318\n","Epoch [45/50] | Loss Train: 0.322064, Loss Test: 0.356737\n","Epoch [46/50] | Loss Train: 0.319281, Loss Test: 0.347511\n","Epoch [47/50] | Loss Train: 0.316403, Loss Test: 0.341518\n","Epoch [48/50] | Loss Train: 0.316167, Loss Test: 0.349827\n","Epoch [49/50] | Loss Train: 0.316952, Loss Test: 0.344278\n","Epoch [50/50] | Loss Train: 0.320576, Loss Test: 0.340601\n","[27/36] (Epochs: 50, Lr: 0.1, Hidden_dim: 128) =>\n","    best_model_score: 0.8626201923076923 - train_score: 0.8581330128205128\n","Epoch [1/50] | Loss Train: 0.430560, Loss Test: 0.355769\n","Epoch [2/50] | Loss Train: 0.365212, Loss Test: 0.351650\n","Epoch [3/50] | Loss Train: 0.353335, Loss Test: 0.335898\n","Epoch [4/50] | Loss Train: 0.352782, Loss Test: 0.339310\n","Epoch [5/50] | Loss Train: 0.345591, Loss Test: 0.331255\n","Epoch [6/50] | Loss Train: 0.345928, Loss Test: 0.346930\n","Epoch [7/50] | Loss Train: 0.352311, Loss Test: 0.340799\n","Epoch [8/50] | Loss Train: 0.342398, Loss Test: 0.338527\n","Epoch [9/50] | Loss Train: 0.334798, Loss Test: 0.340760\n","Epoch [10/50] | Loss Train: 0.334574, Loss Test: 0.330478\n","Epoch [11/50] | Loss Train: 0.332095, Loss Test: 0.344735\n","Epoch [12/50] | Loss Train: 0.332353, Loss Test: 0.339432\n","Epoch [13/50] | Loss Train: 0.330918, Loss Test: 0.333342\n","Epoch [14/50] | Loss Train: 0.338963, Loss Test: 0.453558\n","Epoch [15/50] | Loss Train: 0.330897, Loss Test: 0.333375\n","Epoch [16/50] | Loss Train: 0.329648, Loss Test: 0.335375\n","Epoch [17/50] | Loss Train: 0.331410, Loss Test: 0.331804\n","Epoch [18/50] | Loss Train: 0.329191, Loss Test: 0.375385\n","Epoch [19/50] | Loss Train: 0.327482, Loss Test: 0.339992\n","Epoch [20/50] | Loss Train: 0.332158, Loss Test: 0.329214\n","Epoch [21/50] | Loss Train: 0.325978, Loss Test: 0.332749\n","Epoch [22/50] | Loss Train: 0.330504, Loss Test: 0.341333\n","Epoch [23/50] | Loss Train: 0.327884, Loss Test: 0.349050\n","Epoch [24/50] | Loss Train: 0.328314, Loss Test: 0.339533\n","Epoch [25/50] | Loss Train: 0.327999, Loss Test: 0.331087\n","Epoch [26/50] | Loss Train: 0.324356, Loss Test: 0.340620\n","Epoch [27/50] | Loss Train: 0.326264, Loss Test: 0.329538\n","Epoch [28/50] | Loss Train: 0.322441, Loss Test: 0.355200\n","Epoch [29/50] | Loss Train: 0.328143, Loss Test: 0.345148\n","Epoch [30/50] | Loss Train: 0.325136, Loss Test: 0.346107\n","Epoch [31/50] | Loss Train: 0.324052, Loss Test: 0.331081\n","Epoch [32/50] | Loss Train: 0.320828, Loss Test: 0.336036\n","Epoch [33/50] | Loss Train: 0.326201, Loss Test: 0.333905\n","Epoch [34/50] | Loss Train: 0.322820, Loss Test: 0.339682\n","Epoch [35/50] | Loss Train: 0.319391, Loss Test: 0.367257\n","Epoch [36/50] | Loss Train: 0.323394, Loss Test: 0.339250\n","Epoch [37/50] | Loss Train: 0.325070, Loss Test: 0.339225\n","Epoch [38/50] | Loss Train: 0.326028, Loss Test: 0.342589\n","Epoch [39/50] | Loss Train: 0.318228, Loss Test: 0.334563\n","Epoch [40/50] | Loss Train: 0.324600, Loss Test: 0.336090\n","Epoch [41/50] | Loss Train: 0.329663, Loss Test: 0.340265\n","Epoch [42/50] | Loss Train: 0.320513, Loss Test: 0.339651\n","Epoch [43/50] | Loss Train: 0.325283, Loss Test: 0.329836\n","Epoch [44/50] | Loss Train: 0.322350, Loss Test: 0.333722\n","Epoch [45/50] | Loss Train: 0.322117, Loss Test: 0.333160\n","Epoch [46/50] | Loss Train: 0.321293, Loss Test: 0.338021\n","Epoch [47/50] | Loss Train: 0.323373, Loss Test: 0.341619\n","Epoch [48/50] | Loss Train: 0.321733, Loss Test: 0.337251\n","Epoch [49/50] | Loss Train: 0.321218, Loss Test: 0.378387\n","Epoch [50/50] | Loss Train: 0.325699, Loss Test: 0.356570\n","[28/36] (Epochs: 50, Lr: 0.1, Hidden_dim: 256) =>\n","    best_model_score: 0.8626201923076923 - train_score: 0.8548878205128205\n","Epoch [1/50] | Loss Train: 0.448933, Loss Test: 0.363957\n","Epoch [2/50] | Loss Train: 0.362240, Loss Test: 0.351796\n","Epoch [3/50] | Loss Train: 0.353449, Loss Test: 0.342847\n","Epoch [4/50] | Loss Train: 0.346589, Loss Test: 0.347746\n","Epoch [5/50] | Loss Train: 0.343636, Loss Test: 0.340120\n","Epoch [6/50] | Loss Train: 0.338842, Loss Test: 0.351991\n","Epoch [7/50] | Loss Train: 0.336956, Loss Test: 0.358544\n","Epoch [8/50] | Loss Train: 0.336385, Loss Test: 0.344229\n","Epoch [9/50] | Loss Train: 0.332980, Loss Test: 0.352618\n","Epoch [10/50] | Loss Train: 0.330511, Loss Test: 0.337548\n","Epoch [11/50] | Loss Train: 0.327758, Loss Test: 0.346357\n","Epoch [12/50] | Loss Train: 0.328299, Loss Test: 0.348866\n","Epoch [13/50] | Loss Train: 0.330002, Loss Test: 0.330404\n","Epoch [14/50] | Loss Train: 0.325137, Loss Test: 0.378987\n","Epoch [15/50] | Loss Train: 0.326029, Loss Test: 0.333629\n","Epoch [16/50] | Loss Train: 0.325765, Loss Test: 0.328028\n","Epoch [17/50] | Loss Train: 0.327054, Loss Test: 0.331563\n","Epoch [18/50] | Loss Train: 0.323591, Loss Test: 0.328474\n","Epoch [19/50] | Loss Train: 0.325202, Loss Test: 0.331334\n","Epoch [20/50] | Loss Train: 0.320884, Loss Test: 0.331574\n","Epoch [21/50] | Loss Train: 0.324956, Loss Test: 0.326048\n","Epoch [22/50] | Loss Train: 0.323432, Loss Test: 0.339421\n","Epoch [23/50] | Loss Train: 0.321762, Loss Test: 0.342900\n","Epoch [24/50] | Loss Train: 0.324288, Loss Test: 0.328965\n","Epoch [25/50] | Loss Train: 0.319153, Loss Test: 0.331145\n","Epoch [26/50] | Loss Train: 0.318678, Loss Test: 0.350235\n","Epoch [27/50] | Loss Train: 0.318896, Loss Test: 0.351503\n","Epoch [28/50] | Loss Train: 0.319680, Loss Test: 0.325365\n","Epoch [29/50] | Loss Train: 0.320008, Loss Test: 0.329431\n","Epoch [30/50] | Loss Train: 0.318596, Loss Test: 0.332390\n","Epoch [31/50] | Loss Train: 0.317028, Loss Test: 0.328609\n","Epoch [32/50] | Loss Train: 0.315408, Loss Test: 0.408348\n","Epoch [33/50] | Loss Train: 0.318377, Loss Test: 0.335949\n","Epoch [34/50] | Loss Train: 0.314092, Loss Test: 0.341954\n","Epoch [35/50] | Loss Train: 0.313634, Loss Test: 0.342472\n","Epoch [36/50] | Loss Train: 0.314808, Loss Test: 0.341386\n","Epoch [37/50] | Loss Train: 0.313621, Loss Test: 0.331463\n","Epoch [38/50] | Loss Train: 0.313597, Loss Test: 0.330088\n","Epoch [39/50] | Loss Train: 0.313459, Loss Test: 0.337081\n","Epoch [40/50] | Loss Train: 0.314020, Loss Test: 0.350376\n","Epoch [41/50] | Loss Train: 0.314441, Loss Test: 0.329848\n","Epoch [42/50] | Loss Train: 0.314031, Loss Test: 0.380227\n","Epoch [43/50] | Loss Train: 0.312592, Loss Test: 0.329086\n","Epoch [44/50] | Loss Train: 0.313141, Loss Test: 0.351784\n","Epoch [45/50] | Loss Train: 0.311520, Loss Test: 0.330068\n","Epoch [46/50] | Loss Train: 0.309051, Loss Test: 0.337356\n","Epoch [47/50] | Loss Train: 0.312283, Loss Test: 0.332485\n","Epoch [48/50] | Loss Train: 0.315185, Loss Test: 0.330174\n","Epoch [49/50] | Loss Train: 0.307943, Loss Test: 0.332649\n","Epoch [50/50] | Loss Train: 0.308891, Loss Test: 0.330495\n","[29/36] (Epochs: 50, Lr: 0.01, Hidden_dim: 32) =>\n","    best_model_score: 0.8626201923076923 - train_score: 0.8634214743589743\n"," new best score: 0.8634214743589743\n","Epoch [1/50] | Loss Train: 0.421076, Loss Test: 0.357443\n","Epoch [2/50] | Loss Train: 0.356351, Loss Test: 0.379415\n","Epoch [3/50] | Loss Train: 0.349556, Loss Test: 0.363144\n","Epoch [4/50] | Loss Train: 0.345968, Loss Test: 0.338830\n","Epoch [5/50] | Loss Train: 0.344574, Loss Test: 0.337693\n","Epoch [6/50] | Loss Train: 0.341218, Loss Test: 0.337948\n","Epoch [7/50] | Loss Train: 0.337113, Loss Test: 0.340244\n","Epoch [8/50] | Loss Train: 0.337227, Loss Test: 0.333068\n","Epoch [9/50] | Loss Train: 0.335441, Loss Test: 0.351205\n","Epoch [10/50] | Loss Train: 0.334073, Loss Test: 0.333476\n","Epoch [11/50] | Loss Train: 0.327622, Loss Test: 0.333733\n","Epoch [12/50] | Loss Train: 0.328426, Loss Test: 0.331363\n","Epoch [13/50] | Loss Train: 0.333256, Loss Test: 0.332370\n","Epoch [14/50] | Loss Train: 0.324145, Loss Test: 0.326391\n","Epoch [15/50] | Loss Train: 0.325605, Loss Test: 0.327586\n","Epoch [16/50] | Loss Train: 0.323162, Loss Test: 0.336305\n","Epoch [17/50] | Loss Train: 0.326939, Loss Test: 0.327759\n","Epoch [18/50] | Loss Train: 0.320693, Loss Test: 0.326427\n","Epoch [19/50] | Loss Train: 0.319633, Loss Test: 0.361914\n","Epoch [20/50] | Loss Train: 0.322130, Loss Test: 0.338296\n","Epoch [21/50] | Loss Train: 0.317935, Loss Test: 0.334616\n","Epoch [22/50] | Loss Train: 0.313705, Loss Test: 0.345611\n","Epoch [23/50] | Loss Train: 0.314792, Loss Test: 0.325021\n","Epoch [24/50] | Loss Train: 0.314039, Loss Test: 0.333961\n","Epoch [25/50] | Loss Train: 0.312417, Loss Test: 0.323958\n","Epoch [26/50] | Loss Train: 0.312863, Loss Test: 0.329850\n","Epoch [27/50] | Loss Train: 0.308270, Loss Test: 0.326367\n","Epoch [28/50] | Loss Train: 0.306596, Loss Test: 0.340359\n","Epoch [29/50] | Loss Train: 0.306236, Loss Test: 0.334351\n","Epoch [30/50] | Loss Train: 0.308358, Loss Test: 0.334483\n","Epoch [31/50] | Loss Train: 0.302252, Loss Test: 0.340965\n","Epoch [32/50] | Loss Train: 0.305278, Loss Test: 0.324461\n","Epoch [33/50] | Loss Train: 0.304428, Loss Test: 0.332596\n","Epoch [34/50] | Loss Train: 0.304882, Loss Test: 0.353844\n","Epoch [35/50] | Loss Train: 0.300431, Loss Test: 0.341369\n","Epoch [36/50] | Loss Train: 0.299123, Loss Test: 0.327399\n","Epoch [37/50] | Loss Train: 0.297572, Loss Test: 0.342076\n","Epoch [38/50] | Loss Train: 0.298632, Loss Test: 0.330198\n","Epoch [39/50] | Loss Train: 0.298236, Loss Test: 0.353634\n","Epoch [40/50] | Loss Train: 0.295392, Loss Test: 0.329358\n","Epoch [41/50] | Loss Train: 0.293842, Loss Test: 0.334651\n","Epoch [42/50] | Loss Train: 0.293348, Loss Test: 0.331536\n","Epoch [43/50] | Loss Train: 0.290732, Loss Test: 0.334986\n","Epoch [44/50] | Loss Train: 0.291639, Loss Test: 0.333189\n","Epoch [45/50] | Loss Train: 0.290981, Loss Test: 0.337404\n","Epoch [46/50] | Loss Train: 0.290993, Loss Test: 0.336392\n","Epoch [47/50] | Loss Train: 0.285772, Loss Test: 0.334751\n","Epoch [48/50] | Loss Train: 0.285683, Loss Test: 0.374082\n","Epoch [49/50] | Loss Train: 0.287346, Loss Test: 0.337551\n","Epoch [50/50] | Loss Train: 0.285927, Loss Test: 0.334724\n","[30/36] (Epochs: 50, Lr: 0.01, Hidden_dim: 64) =>\n","    best_model_score: 0.8634214743589743 - train_score: 0.8605769230769231\n","Epoch [1/50] | Loss Train: 0.421501, Loss Test: 0.359774\n","Epoch [2/50] | Loss Train: 0.359697, Loss Test: 0.350713\n","Epoch [3/50] | Loss Train: 0.349249, Loss Test: 0.357750\n","Epoch [4/50] | Loss Train: 0.347040, Loss Test: 0.341097\n","Epoch [5/50] | Loss Train: 0.343585, Loss Test: 0.336286\n","Epoch [6/50] | Loss Train: 0.336629, Loss Test: 0.342785\n","Epoch [7/50] | Loss Train: 0.338297, Loss Test: 0.334838\n","Epoch [8/50] | Loss Train: 0.338108, Loss Test: 0.354657\n","Epoch [9/50] | Loss Train: 0.334582, Loss Test: 0.345944\n","Epoch [10/50] | Loss Train: 0.336385, Loss Test: 0.346139\n","Epoch [11/50] | Loss Train: 0.336568, Loss Test: 0.341588\n","Epoch [12/50] | Loss Train: 0.334475, Loss Test: 0.338784\n","Epoch [13/50] | Loss Train: 0.329926, Loss Test: 0.336295\n","Epoch [14/50] | Loss Train: 0.329831, Loss Test: 0.332996\n","Epoch [15/50] | Loss Train: 0.332313, Loss Test: 0.353483\n","Epoch [16/50] | Loss Train: 0.327021, Loss Test: 0.353058\n","Epoch [17/50] | Loss Train: 0.328930, Loss Test: 0.354475\n","Epoch [18/50] | Loss Train: 0.328851, Loss Test: 0.327325\n","Epoch [19/50] | Loss Train: 0.325064, Loss Test: 0.332877\n","Epoch [20/50] | Loss Train: 0.322674, Loss Test: 0.327766\n","Epoch [21/50] | Loss Train: 0.322696, Loss Test: 0.336724\n","Epoch [22/50] | Loss Train: 0.319758, Loss Test: 0.329657\n","Epoch [23/50] | Loss Train: 0.320412, Loss Test: 0.325786\n","Epoch [24/50] | Loss Train: 0.317495, Loss Test: 0.338767\n","Epoch [25/50] | Loss Train: 0.321167, Loss Test: 0.326026\n","Epoch [26/50] | Loss Train: 0.316430, Loss Test: 0.325024\n","Epoch [27/50] | Loss Train: 0.316934, Loss Test: 0.327114\n","Epoch [28/50] | Loss Train: 0.315558, Loss Test: 0.327673\n","Epoch [29/50] | Loss Train: 0.317666, Loss Test: 0.340676\n","Epoch [30/50] | Loss Train: 0.312009, Loss Test: 0.325725\n","Epoch [31/50] | Loss Train: 0.313954, Loss Test: 0.328800\n","Epoch [32/50] | Loss Train: 0.311535, Loss Test: 0.327878\n","Epoch [33/50] | Loss Train: 0.308921, Loss Test: 0.333274\n","Epoch [34/50] | Loss Train: 0.312004, Loss Test: 0.336527\n","Epoch [35/50] | Loss Train: 0.309517, Loss Test: 0.331282\n","Epoch [36/50] | Loss Train: 0.307957, Loss Test: 0.329838\n","Epoch [37/50] | Loss Train: 0.305924, Loss Test: 0.328357\n","Epoch [38/50] | Loss Train: 0.301303, Loss Test: 0.332372\n","Epoch [39/50] | Loss Train: 0.303299, Loss Test: 0.331901\n","Epoch [40/50] | Loss Train: 0.303695, Loss Test: 0.345873\n","Epoch [41/50] | Loss Train: 0.302117, Loss Test: 0.339192\n","Epoch [42/50] | Loss Train: 0.299089, Loss Test: 0.332083\n","Epoch [43/50] | Loss Train: 0.299105, Loss Test: 0.333944\n","Epoch [44/50] | Loss Train: 0.302387, Loss Test: 0.333808\n","Epoch [45/50] | Loss Train: 0.296703, Loss Test: 0.336890\n","Epoch [46/50] | Loss Train: 0.295978, Loss Test: 0.333028\n","Epoch [47/50] | Loss Train: 0.297790, Loss Test: 0.340227\n","Epoch [48/50] | Loss Train: 0.294359, Loss Test: 0.331900\n","Epoch [49/50] | Loss Train: 0.292451, Loss Test: 0.334753\n","Epoch [50/50] | Loss Train: 0.296111, Loss Test: 0.331388\n","[31/36] (Epochs: 50, Lr: 0.01, Hidden_dim: 128) =>\n","    best_model_score: 0.8634214743589743 - train_score: 0.8610977564102564\n","Epoch [1/50] | Loss Train: 0.412608, Loss Test: 0.356135\n","Epoch [2/50] | Loss Train: 0.365200, Loss Test: 0.347730\n","Epoch [3/50] | Loss Train: 0.357134, Loss Test: 0.341984\n","Epoch [4/50] | Loss Train: 0.345632, Loss Test: 0.343181\n","Epoch [5/50] | Loss Train: 0.339585, Loss Test: 0.337598\n","Epoch [6/50] | Loss Train: 0.338526, Loss Test: 0.350394\n","Epoch [7/50] | Loss Train: 0.338934, Loss Test: 0.332794\n","Epoch [8/50] | Loss Train: 0.334679, Loss Test: 0.346098\n","Epoch [9/50] | Loss Train: 0.337815, Loss Test: 0.333230\n","Epoch [10/50] | Loss Train: 0.329877, Loss Test: 0.351513\n","Epoch [11/50] | Loss Train: 0.327663, Loss Test: 0.334255\n","Epoch [12/50] | Loss Train: 0.328757, Loss Test: 0.427756\n","Epoch [13/50] | Loss Train: 0.327970, Loss Test: 0.365034\n","Epoch [14/50] | Loss Train: 0.320181, Loss Test: 0.330954\n","Epoch [15/50] | Loss Train: 0.324368, Loss Test: 0.328278\n","Epoch [16/50] | Loss Train: 0.318490, Loss Test: 0.342151\n","Epoch [17/50] | Loss Train: 0.321924, Loss Test: 0.373194\n","Epoch [18/50] | Loss Train: 0.320600, Loss Test: 0.331310\n","Epoch [19/50] | Loss Train: 0.319022, Loss Test: 0.341560\n","Epoch [20/50] | Loss Train: 0.314346, Loss Test: 0.341432\n","Epoch [21/50] | Loss Train: 0.315377, Loss Test: 0.339024\n","Epoch [22/50] | Loss Train: 0.309336, Loss Test: 0.344549\n","Epoch [23/50] | Loss Train: 0.315311, Loss Test: 0.356604\n","Epoch [24/50] | Loss Train: 0.308565, Loss Test: 0.363145\n","Epoch [25/50] | Loss Train: 0.307735, Loss Test: 0.341547\n","Epoch [26/50] | Loss Train: 0.307736, Loss Test: 0.331979\n","Epoch [27/50] | Loss Train: 0.307052, Loss Test: 0.334668\n","Epoch [28/50] | Loss Train: 0.304012, Loss Test: 0.335165\n","Epoch [29/50] | Loss Train: 0.299500, Loss Test: 0.350729\n","Epoch [30/50] | Loss Train: 0.303561, Loss Test: 0.331020\n","Epoch [31/50] | Loss Train: 0.300902, Loss Test: 0.359502\n","Epoch [32/50] | Loss Train: 0.301975, Loss Test: 0.337827\n","Epoch [33/50] | Loss Train: 0.297743, Loss Test: 0.332012\n","Epoch [34/50] | Loss Train: 0.295678, Loss Test: 0.339174\n","Epoch [35/50] | Loss Train: 0.297257, Loss Test: 0.341216\n","Epoch [36/50] | Loss Train: 0.292881, Loss Test: 0.333751\n","Epoch [37/50] | Loss Train: 0.292586, Loss Test: 0.345588\n","Epoch [38/50] | Loss Train: 0.291622, Loss Test: 0.348683\n","Epoch [39/50] | Loss Train: 0.292116, Loss Test: 0.339065\n","Epoch [40/50] | Loss Train: 0.288940, Loss Test: 0.341076\n","Epoch [41/50] | Loss Train: 0.290277, Loss Test: 0.352652\n","Epoch [42/50] | Loss Train: 0.290552, Loss Test: 0.352747\n","Epoch [43/50] | Loss Train: 0.287692, Loss Test: 0.363300\n","Epoch [44/50] | Loss Train: 0.284937, Loss Test: 0.337162\n","Epoch [45/50] | Loss Train: 0.286038, Loss Test: 0.346120\n","Epoch [46/50] | Loss Train: 0.284202, Loss Test: 0.346228\n","Epoch [47/50] | Loss Train: 0.281211, Loss Test: 0.339258\n","Epoch [48/50] | Loss Train: 0.281123, Loss Test: 0.341090\n","Epoch [49/50] | Loss Train: 0.278426, Loss Test: 0.341523\n","Epoch [50/50] | Loss Train: 0.279658, Loss Test: 0.339796\n","[32/36] (Epochs: 50, Lr: 0.01, Hidden_dim: 256) =>\n","    best_model_score: 0.8634214743589743 - train_score: 0.8588942307692308\n","Epoch [1/50] | Loss Train: 0.603554, Loss Test: 0.489329\n","Epoch [2/50] | Loss Train: 0.434474, Loss Test: 0.404952\n","Epoch [3/50] | Loss Train: 0.388378, Loss Test: 0.381609\n","Epoch [4/50] | Loss Train: 0.371866, Loss Test: 0.375752\n","Epoch [5/50] | Loss Train: 0.362442, Loss Test: 0.360763\n","Epoch [6/50] | Loss Train: 0.354830, Loss Test: 0.356287\n","Epoch [7/50] | Loss Train: 0.350409, Loss Test: 0.351808\n","Epoch [8/50] | Loss Train: 0.347442, Loss Test: 0.351680\n","Epoch [9/50] | Loss Train: 0.344503, Loss Test: 0.357897\n","Epoch [10/50] | Loss Train: 0.341251, Loss Test: 0.349871\n","Epoch [11/50] | Loss Train: 0.340920, Loss Test: 0.342674\n","Epoch [12/50] | Loss Train: 0.338845, Loss Test: 0.344149\n","Epoch [13/50] | Loss Train: 0.338203, Loss Test: 0.345352\n","Epoch [14/50] | Loss Train: 0.336357, Loss Test: 0.339565\n","Epoch [15/50] | Loss Train: 0.334299, Loss Test: 0.344633\n","Epoch [16/50] | Loss Train: 0.333672, Loss Test: 0.341934\n","Epoch [17/50] | Loss Train: 0.332669, Loss Test: 0.336579\n","Epoch [18/50] | Loss Train: 0.331949, Loss Test: 0.343931\n","Epoch [19/50] | Loss Train: 0.330656, Loss Test: 0.336640\n","Epoch [20/50] | Loss Train: 0.330260, Loss Test: 0.335408\n","Epoch [21/50] | Loss Train: 0.329744, Loss Test: 0.334202\n","Epoch [22/50] | Loss Train: 0.328472, Loss Test: 0.333620\n","Epoch [23/50] | Loss Train: 0.328582, Loss Test: 0.334731\n","Epoch [24/50] | Loss Train: 0.327897, Loss Test: 0.336205\n","Epoch [25/50] | Loss Train: 0.327091, Loss Test: 0.335381\n","Epoch [26/50] | Loss Train: 0.325803, Loss Test: 0.333215\n","Epoch [27/50] | Loss Train: 0.326298, Loss Test: 0.333058\n","Epoch [28/50] | Loss Train: 0.325724, Loss Test: 0.331491\n","Epoch [29/50] | Loss Train: 0.323543, Loss Test: 0.331274\n","Epoch [30/50] | Loss Train: 0.324763, Loss Test: 0.332246\n","Epoch [31/50] | Loss Train: 0.323808, Loss Test: 0.331934\n","Epoch [32/50] | Loss Train: 0.322721, Loss Test: 0.329746\n","Epoch [33/50] | Loss Train: 0.323248, Loss Test: 0.330583\n","Epoch [34/50] | Loss Train: 0.322507, Loss Test: 0.330007\n","Epoch [35/50] | Loss Train: 0.321843, Loss Test: 0.329264\n","Epoch [36/50] | Loss Train: 0.322078, Loss Test: 0.328501\n","Epoch [37/50] | Loss Train: 0.320912, Loss Test: 0.328959\n","Epoch [38/50] | Loss Train: 0.319774, Loss Test: 0.330067\n","Epoch [39/50] | Loss Train: 0.320564, Loss Test: 0.328721\n","Epoch [40/50] | Loss Train: 0.319312, Loss Test: 0.328972\n","Epoch [41/50] | Loss Train: 0.319410, Loss Test: 0.327730\n","Epoch [42/50] | Loss Train: 0.318923, Loss Test: 0.332709\n","Epoch [43/50] | Loss Train: 0.319312, Loss Test: 0.327302\n","Epoch [44/50] | Loss Train: 0.318306, Loss Test: 0.327477\n","Epoch [45/50] | Loss Train: 0.319461, Loss Test: 0.327175\n","Epoch [46/50] | Loss Train: 0.318225, Loss Test: 0.328101\n","Epoch [47/50] | Loss Train: 0.317702, Loss Test: 0.328917\n","Epoch [48/50] | Loss Train: 0.317683, Loss Test: 0.327489\n","Epoch [49/50] | Loss Train: 0.316907, Loss Test: 0.327809\n","Epoch [50/50] | Loss Train: 0.316173, Loss Test: 0.327786\n","[33/36] (Epochs: 50, Lr: 0.001, Hidden_dim: 32) =>\n","    best_model_score: 0.8634214743589743 - train_score: 0.8612980769230769\n","Epoch [1/50] | Loss Train: 0.556862, Loss Test: 0.442633\n","Epoch [2/50] | Loss Train: 0.400066, Loss Test: 0.382699\n","Epoch [3/50] | Loss Train: 0.371685, Loss Test: 0.367019\n","Epoch [4/50] | Loss Train: 0.359251, Loss Test: 0.362756\n","Epoch [5/50] | Loss Train: 0.352191, Loss Test: 0.350749\n","Epoch [6/50] | Loss Train: 0.345723, Loss Test: 0.351277\n","Epoch [7/50] | Loss Train: 0.343243, Loss Test: 0.344160\n","Epoch [8/50] | Loss Train: 0.339427, Loss Test: 0.342168\n","Epoch [9/50] | Loss Train: 0.339269, Loss Test: 0.341205\n","Epoch [10/50] | Loss Train: 0.336637, Loss Test: 0.347732\n","Epoch [11/50] | Loss Train: 0.333734, Loss Test: 0.337187\n","Epoch [12/50] | Loss Train: 0.332681, Loss Test: 0.335568\n","Epoch [13/50] | Loss Train: 0.330920, Loss Test: 0.347292\n","Epoch [14/50] | Loss Train: 0.330695, Loss Test: 0.333892\n","Epoch [15/50] | Loss Train: 0.329130, Loss Test: 0.333026\n","Epoch [16/50] | Loss Train: 0.328118, Loss Test: 0.331824\n","Epoch [17/50] | Loss Train: 0.326566, Loss Test: 0.346603\n","Epoch [18/50] | Loss Train: 0.327469, Loss Test: 0.335851\n","Epoch [19/50] | Loss Train: 0.324643, Loss Test: 0.341359\n","Epoch [20/50] | Loss Train: 0.324498, Loss Test: 0.343957\n","Epoch [21/50] | Loss Train: 0.322479, Loss Test: 0.328885\n","Epoch [22/50] | Loss Train: 0.322889, Loss Test: 0.340855\n","Epoch [23/50] | Loss Train: 0.322156, Loss Test: 0.330306\n","Epoch [24/50] | Loss Train: 0.321756, Loss Test: 0.327832\n","Epoch [25/50] | Loss Train: 0.319815, Loss Test: 0.327409\n","Epoch [26/50] | Loss Train: 0.319296, Loss Test: 0.331448\n","Epoch [27/50] | Loss Train: 0.319712, Loss Test: 0.327338\n","Epoch [28/50] | Loss Train: 0.320230, Loss Test: 0.335687\n","Epoch [29/50] | Loss Train: 0.318051, Loss Test: 0.335890\n","Epoch [30/50] | Loss Train: 0.317679, Loss Test: 0.327948\n","Epoch [31/50] | Loss Train: 0.318108, Loss Test: 0.328651\n","Epoch [32/50] | Loss Train: 0.315667, Loss Test: 0.328251\n","Epoch [33/50] | Loss Train: 0.315169, Loss Test: 0.330713\n","Epoch [34/50] | Loss Train: 0.316429, Loss Test: 0.327116\n","Epoch [35/50] | Loss Train: 0.314889, Loss Test: 0.328666\n","Epoch [36/50] | Loss Train: 0.314463, Loss Test: 0.326655\n","Epoch [37/50] | Loss Train: 0.313716, Loss Test: 0.327013\n","Epoch [38/50] | Loss Train: 0.313242, Loss Test: 0.326219\n","Epoch [39/50] | Loss Train: 0.311288, Loss Test: 0.329330\n","Epoch [40/50] | Loss Train: 0.313637, Loss Test: 0.326390\n","Epoch [41/50] | Loss Train: 0.310296, Loss Test: 0.328823\n","Epoch [42/50] | Loss Train: 0.311481, Loss Test: 0.325315\n","Epoch [43/50] | Loss Train: 0.311334, Loss Test: 0.324196\n","Epoch [44/50] | Loss Train: 0.310176, Loss Test: 0.331815\n","Epoch [45/50] | Loss Train: 0.308117, Loss Test: 0.344706\n","Epoch [46/50] | Loss Train: 0.309551, Loss Test: 0.327486\n","Epoch [47/50] | Loss Train: 0.307734, Loss Test: 0.324017\n","Epoch [48/50] | Loss Train: 0.307743, Loss Test: 0.331672\n","Epoch [49/50] | Loss Train: 0.307128, Loss Test: 0.325224\n","Epoch [50/50] | Loss Train: 0.306084, Loss Test: 0.324620\n","[34/36] (Epochs: 50, Lr: 0.001, Hidden_dim: 64) =>\n","    best_model_score: 0.8634214743589743 - train_score: 0.8622596153846154\n","Epoch [1/50] | Loss Train: 0.535759, Loss Test: 0.412758\n","Epoch [2/50] | Loss Train: 0.385750, Loss Test: 0.371911\n","Epoch [3/50] | Loss Train: 0.364528, Loss Test: 0.357721\n","Epoch [4/50] | Loss Train: 0.354319, Loss Test: 0.353937\n","Epoch [5/50] | Loss Train: 0.347484, Loss Test: 0.346770\n","Epoch [6/50] | Loss Train: 0.343620, Loss Test: 0.342635\n","Epoch [7/50] | Loss Train: 0.340326, Loss Test: 0.341184\n","Epoch [8/50] | Loss Train: 0.336367, Loss Test: 0.338844\n","Epoch [9/50] | Loss Train: 0.336346, Loss Test: 0.337247\n","Epoch [10/50] | Loss Train: 0.333399, Loss Test: 0.335749\n","Epoch [11/50] | Loss Train: 0.331262, Loss Test: 0.335180\n","Epoch [12/50] | Loss Train: 0.329872, Loss Test: 0.332844\n","Epoch [13/50] | Loss Train: 0.328505, Loss Test: 0.334601\n","Epoch [14/50] | Loss Train: 0.327073, Loss Test: 0.333288\n","Epoch [15/50] | Loss Train: 0.326201, Loss Test: 0.330319\n","Epoch [16/50] | Loss Train: 0.325433, Loss Test: 0.329242\n","Epoch [17/50] | Loss Train: 0.324568, Loss Test: 0.331581\n","Epoch [18/50] | Loss Train: 0.322037, Loss Test: 0.328171\n","Epoch [19/50] | Loss Train: 0.323811, Loss Test: 0.330436\n","Epoch [20/50] | Loss Train: 0.321441, Loss Test: 0.340065\n","Epoch [21/50] | Loss Train: 0.321767, Loss Test: 0.328763\n","Epoch [22/50] | Loss Train: 0.319495, Loss Test: 0.327265\n","Epoch [23/50] | Loss Train: 0.318882, Loss Test: 0.328372\n","Epoch [24/50] | Loss Train: 0.317706, Loss Test: 0.330576\n","Epoch [25/50] | Loss Train: 0.318540, Loss Test: 0.325364\n","Epoch [26/50] | Loss Train: 0.315336, Loss Test: 0.339054\n","Epoch [27/50] | Loss Train: 0.315280, Loss Test: 0.330922\n","Epoch [28/50] | Loss Train: 0.314594, Loss Test: 0.324649\n","Epoch [29/50] | Loss Train: 0.313535, Loss Test: 0.326031\n","Epoch [30/50] | Loss Train: 0.312920, Loss Test: 0.337825\n","Epoch [31/50] | Loss Train: 0.313836, Loss Test: 0.329383\n","Epoch [32/50] | Loss Train: 0.311590, Loss Test: 0.333527\n","Epoch [33/50] | Loss Train: 0.311354, Loss Test: 0.326909\n","Epoch [34/50] | Loss Train: 0.310898, Loss Test: 0.325789\n","Epoch [35/50] | Loss Train: 0.310600, Loss Test: 0.337725\n","Epoch [36/50] | Loss Train: 0.309586, Loss Test: 0.324141\n","Epoch [37/50] | Loss Train: 0.307277, Loss Test: 0.324227\n","Epoch [38/50] | Loss Train: 0.306720, Loss Test: 0.327204\n","Epoch [39/50] | Loss Train: 0.305230, Loss Test: 0.323655\n","Epoch [40/50] | Loss Train: 0.305617, Loss Test: 0.323552\n","Epoch [41/50] | Loss Train: 0.303688, Loss Test: 0.322836\n","Epoch [42/50] | Loss Train: 0.304053, Loss Test: 0.326294\n","Epoch [43/50] | Loss Train: 0.303722, Loss Test: 0.339630\n","Epoch [44/50] | Loss Train: 0.300746, Loss Test: 0.329204\n","Epoch [45/50] | Loss Train: 0.302262, Loss Test: 0.323089\n","Epoch [46/50] | Loss Train: 0.300207, Loss Test: 0.322411\n","Epoch [47/50] | Loss Train: 0.299336, Loss Test: 0.326169\n","Epoch [48/50] | Loss Train: 0.298580, Loss Test: 0.327392\n","Epoch [49/50] | Loss Train: 0.297599, Loss Test: 0.322533\n","Epoch [50/50] | Loss Train: 0.296442, Loss Test: 0.327656\n","[35/36] (Epochs: 50, Lr: 0.001, Hidden_dim: 128) =>\n","    best_model_score: 0.8634214743589743 - train_score: 0.8610176282051282\n","Epoch [1/50] | Loss Train: 0.494811, Loss Test: 0.388920\n","Epoch [2/50] | Loss Train: 0.374732, Loss Test: 0.366220\n","Epoch [3/50] | Loss Train: 0.355764, Loss Test: 0.352120\n","Epoch [4/50] | Loss Train: 0.349155, Loss Test: 0.348909\n","Epoch [5/50] | Loss Train: 0.344467, Loss Test: 0.343608\n","Epoch [6/50] | Loss Train: 0.339242, Loss Test: 0.340596\n","Epoch [7/50] | Loss Train: 0.335221, Loss Test: 0.341464\n","Epoch [8/50] | Loss Train: 0.335904, Loss Test: 0.342923\n","Epoch [9/50] | Loss Train: 0.333033, Loss Test: 0.333960\n","Epoch [10/50] | Loss Train: 0.331226, Loss Test: 0.340868\n","Epoch [11/50] | Loss Train: 0.330136, Loss Test: 0.337575\n","Epoch [12/50] | Loss Train: 0.327170, Loss Test: 0.339771\n","Epoch [13/50] | Loss Train: 0.327542, Loss Test: 0.329804\n","Epoch [14/50] | Loss Train: 0.324533, Loss Test: 0.330115\n","Epoch [15/50] | Loss Train: 0.323385, Loss Test: 0.328823\n","Epoch [16/50] | Loss Train: 0.323415, Loss Test: 0.328528\n","Epoch [17/50] | Loss Train: 0.320700, Loss Test: 0.338138\n","Epoch [18/50] | Loss Train: 0.320211, Loss Test: 0.331489\n","Epoch [19/50] | Loss Train: 0.320926, Loss Test: 0.356084\n","Epoch [20/50] | Loss Train: 0.321314, Loss Test: 0.327970\n","Epoch [21/50] | Loss Train: 0.317682, Loss Test: 0.337286\n","Epoch [22/50] | Loss Train: 0.318230, Loss Test: 0.328453\n","Epoch [23/50] | Loss Train: 0.316354, Loss Test: 0.328907\n","Epoch [24/50] | Loss Train: 0.314143, Loss Test: 0.324510\n","Epoch [25/50] | Loss Train: 0.313240, Loss Test: 0.326705\n","Epoch [26/50] | Loss Train: 0.313058, Loss Test: 0.328272\n","Epoch [27/50] | Loss Train: 0.311689, Loss Test: 0.325816\n","Epoch [28/50] | Loss Train: 0.309432, Loss Test: 0.329534\n","Epoch [29/50] | Loss Train: 0.310211, Loss Test: 0.326368\n","Epoch [30/50] | Loss Train: 0.308617, Loss Test: 0.322344\n","Epoch [31/50] | Loss Train: 0.307597, Loss Test: 0.323065\n","Epoch [32/50] | Loss Train: 0.305585, Loss Test: 0.323811\n","Epoch [33/50] | Loss Train: 0.305038, Loss Test: 0.326280\n","Epoch [34/50] | Loss Train: 0.306098, Loss Test: 0.352777\n","Epoch [35/50] | Loss Train: 0.304011, Loss Test: 0.323841\n","Epoch [36/50] | Loss Train: 0.301161, Loss Test: 0.322151\n","Epoch [37/50] | Loss Train: 0.302228, Loss Test: 0.329425\n","Epoch [38/50] | Loss Train: 0.301259, Loss Test: 0.321883\n","Epoch [39/50] | Loss Train: 0.299153, Loss Test: 0.323085\n","Epoch [40/50] | Loss Train: 0.298419, Loss Test: 0.325431\n","Epoch [41/50] | Loss Train: 0.297324, Loss Test: 0.326557\n","Epoch [42/50] | Loss Train: 0.294352, Loss Test: 0.328564\n","Epoch [43/50] | Loss Train: 0.296373, Loss Test: 0.332355\n","Epoch [44/50] | Loss Train: 0.294972, Loss Test: 0.325722\n","Epoch [45/50] | Loss Train: 0.292499, Loss Test: 0.321873\n","Epoch [46/50] | Loss Train: 0.292141, Loss Test: 0.327516\n","Epoch [47/50] | Loss Train: 0.289545, Loss Test: 0.327325\n","Epoch [48/50] | Loss Train: 0.290238, Loss Test: 0.327096\n","Epoch [49/50] | Loss Train: 0.288934, Loss Test: 0.326522\n","Epoch [50/50] | Loss Train: 0.286658, Loss Test: 0.326944\n","[36/36] (Epochs: 50, Lr: 0.001, Hidden_dim: 256) =>\n","    best_model_score: 0.8634214743589743 - train_score: 0.8614583333333333\n","[Grid Search] best combination (epochs, lr, hidden_dim): (50, 0.01, 32)\n"]}]},{"cell_type":"markdown","source":["## Perform an evaluation"],"metadata":{"id":"U0Df2D5mSXaS"}},{"cell_type":"code","source":["with torch.no_grad():\n","    correct, total = 0, 0\n","\n","    for labels, texts in test_loader:\n","        labels = labels.to(device).unsqueeze(1)\n","        texts = texts.to(device)\n","        outputs = best_model(texts)\n","        predictions = torch.round(outputs.data)\n","        total += labels.size(0)\n","        correct += (predictions == labels).sum().item()\n","\n","    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJDQwmYRSaXt","executionInfo":{"status":"ok","timestamp":1695478661002,"user_tz":180,"elapsed":19,"user":{"displayName":"Jose Cassano","userId":"06274683169262378241"}},"outputId":"079e9877-1218-48b4-a161-48ab13d58a52"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 86.34%\n"]}]},{"cell_type":"markdown","source":["# TODO:\n","1. [Done] Check why it doesn't work. (It was only loading a few negative examples, which allowed learning a trivial model).\n","2. [Done] Do hyper-parameter tuning with grid-search over the learning rate and the MLP hidden dimension.\n","  \n","  (Including epochs)\n","3. [Optional] Use fastText instead of word2vec."],"metadata":{"id":"f-I97cTyWBVs"}}]}